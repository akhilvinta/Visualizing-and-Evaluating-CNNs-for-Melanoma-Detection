{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Installing Dependencies and Packages"
      ],
      "metadata": {
        "id": "FXSKJQEqoG-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install data_reader\n",
        "from data_reader import *\n",
        "from torchvision import transforms\n",
        "import csv\n",
        "import torch as torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyOog8sMDpU6",
        "outputId": "f3db426d-85aa-480f-89f6-97715ac85ce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting data_reader\n",
            "  Downloading data_reader-0.0.8-py3-none-any.whl (6.0 kB)\n",
            "Collecting ijson\n",
            "  Downloading ijson-3.2.0.post0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ijson, data_reader\n",
            "Successfully installed data_reader-0.0.8 ijson-3.2.0.post0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading Data, Labels"
      ],
      "metadata": {
        "id": "7pUmXFiEDl2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "CS188_path = './'\n",
        "os.makedirs(os.path.join(CS188_path, 'FinalProject'), exist_ok=True)\n",
        "root_dir = os.path.join(CS188_path, 'FinalProject')"
      ],
      "metadata": {
        "id": "Z4fKe3LuFzKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rdHKGmw1soFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "from zipfile import ZipFile\n",
        "#Download Training and Validation CSV\n",
        "!gdown 1xUeYl1odxxlgiCo9OGimvB3L6k4_uN9N\n",
        "!gdown 1o62AjBbs-itqAHULcZeiEGRGRudMRVom\n",
        "#Download img training and validation data zipfile with 10015 images -- is taking me ~10-30 seconds\n",
        "!gdown 1fZGdows23YXLq-XiJgBVbLqyD_KlIUnV\n",
        "!gdown 1sciwDsAlNWkVg_qIK-nRijeNMlqEAroH\n",
        "#Move files to ./FinalProject folder\n",
        "shutil.move(\"./ISIC2018_Task3_Training_GroundTruth.csv\", \"./FinalProject/ISIC2018_Task3_Training_GroundTruth.csv\")\n",
        "shutil.move(\"./ISIC2018_Task3_Validation_GroundTruth.csv\", \"./FinalProject/ISIC2018_Task3_Validation_GroundTruth.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "e8Ey41Kvrhpp",
        "outputId": "0c88881e-45fa-4554-e819-dc95e70e7bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xUeYl1odxxlgiCo9OGimvB3L6k4_uN9N\n",
            "To: /content/ISIC2018_Task3_Training_GroundTruth.csv\n",
            "100% 421k/421k [00:00<00:00, 147MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1o62AjBbs-itqAHULcZeiEGRGRudMRVom\n",
            "To: /content/ISIC2018_Task3_Validation_GroundTruth.csv\n",
            "100% 8.14k/8.14k [00:00<00:00, 13.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1fZGdows23YXLq-XiJgBVbLqyD_KlIUnV\n",
            "To: /content/ISIC2018_Task3_Training_Input.zip\n",
            "100% 2.78G/2.78G [00:32<00:00, 85.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sciwDsAlNWkVg_qIK-nRijeNMlqEAroH\n",
            "To: /content/ISIC2018_Task3_Validation_Input.zip\n",
            "100% 53.4M/53.4M [00:01<00:00, 28.7MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./FinalProject/ISIC2018_Task3_Validation_GroundTruth.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "train_file_name = \"ISIC2018_Task3_Training_Input.zip\"\n",
        "val_file_name = \"ISIC2018_Task3_Validation_Input.zip\"\n",
        "def unzip(source_filename, dest_dir):\n",
        "    with ZipFile(source_filename) as zf:\n",
        "        zf.extractall(dest_dir)\n",
        "\n",
        "#Unzip all 10,015 images to \"./FinalProject/ISIC2018_Task3_Training_Input\" and \"./FinalProject/ISIC2018_Task3_Validation_Input\"\n",
        "#Taking me ~13 seconds\n",
        "unzip(train_file_name, \"./FinalProject/\")\n",
        "unzip(val_file_name, \"./FinalProject/\")\n"
      ],
      "metadata": {
        "id": "vogzWdAgjo-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_directory = {0: 'MEL', 1: 'NV', 2: 'BCC', 3: 'AKIEC', 4: 'BKL', 5: 'DF', 6: 'VASC'}"
      ],
      "metadata": {
        "id": "uJuJhVjNIysp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SkinLesions(Dataset):\n",
        "    def __init__(self, root_dir, foldername, label_dict = None, transform=None, size = None):\n",
        "        self.root_dir = root_dir\n",
        "        self.foldername = foldername\n",
        "        self.transform = transform\n",
        "        self.filenames = []\n",
        "        self.labels = []\n",
        "        csvfile=os.path.join(root_dir, foldername)\n",
        "\n",
        "        if foldername == 'ISIC2018_Task3_Validation_GroundTruth.csv':\n",
        "          image_folder = 'ISIC2018_Task3_Validation_Input'\n",
        "        else:\n",
        "          image_folder = 'ISIC2018_Task3_Training_Input'\n",
        "\n",
        "        i = 0\n",
        "        with open(csvfile, 'r') as csvfile:\n",
        "          csvreader = csv.reader(csvfile, delimiter = ',')\n",
        "          for line in csvreader:\n",
        "            if i==0:\n",
        "                pass\n",
        "            else:\n",
        "                img_path=os.path.join(root_dir, image_folder, line[0] + \".jpg\")\n",
        "                if os.path.exists(img_path):\n",
        "                    self.filenames.append(img_path)\n",
        "                    self.labels.append(np.argmax(line[1:]))\n",
        "                else:\n",
        "                    print(\"{} does not exists\".format(img_path))\n",
        "                    exit(0)\n",
        "                if size:\n",
        "                  if i == size:\n",
        "                    break\n",
        "            i+=1\n",
        "\n",
        "    def __len__(self):\n",
        "        dataset_len = len(self.labels)\n",
        "        return dataset_len\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.filenames[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        img_full_path = os.path.join(img_path)\n",
        "\n",
        "        image = Image.open(img_full_path)\n",
        "        image = data_transform(image)\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "SAZ0PWxuInH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transforms for the data"
      ],
      "metadata": {
        "id": "vo0GXpI2KXsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n"
      ],
      "metadata": {
        "id": "pyAg0X62BGWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_to_image(image):\n",
        "    tensor_image = image.clone().detach()\n",
        "    inv_norm = transforms.Normalize((-0.485/0.229, -0.456/0.224, -0.406/0.255), (1/0.229, 1/0.224, 1/0.255))\n",
        "    inv_tensor = inv_norm(image)\n",
        "    tensor_transposed = inv_tensor.permute(1, 2, 0)\n",
        "    tensor_clamped = torch.clamp(tensor_transposed, min = 0, max = 255)\n",
        "    image = tensor_clamped.numpy()\n",
        "    return image"
      ],
      "metadata": {
        "id": "HAbIk60DC3m-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Constructions\n",
        "\n",
        "In this section, we will intialize the various models used for the project along with training and evaluating functions.\n",
        "\n"
      ],
      "metadata": {
        "id": "yPBCnvJ0E1vY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device to use for training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device == torch.device('cuda'):\n",
        "    print(f'Using device: {device}. Good to go!')\n",
        "else:\n",
        "    print('Please set GPU via Edit -> Notebook Settings.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LDWVa_WHofm",
        "outputId": "d39afde1-5c9f-4d4a-86ac-249de60c371e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda. Good to go!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs):\n",
        "    # Place model on device\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        # Use tqdm to display a progress bar during training\n",
        "        with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{num_epochs}') as pbar:\n",
        "            for inputs, labels in train_loader:\n",
        "                # Move inputs and labels to device\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # Zero out gradients\n",
        "                optimizer.zero_grad()\n",
        "                # Compute the logits and loss\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, labels)\n",
        "                # Backpropagate the loss\n",
        "                loss.backward()\n",
        "                # Update the weights\n",
        "                optimizer.step()\n",
        "                # Update the progress bar\n",
        "                pbar.update(1)\n",
        "                pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Evaluate the model on the validation set\n",
        "        avg_loss, accuracy = evaluate(model, val_loader, criterion, device)\n",
        "        print(f'Validation set: Average loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}')\n",
        "\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0.0\n",
        "        num_correct = 0\n",
        "        num_samples = 0\n",
        "\n",
        "        for inputs, labels in test_loader:\n",
        "            # Move inputs and labels to device\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Compute the logits and loss\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Compute the accuracy\n",
        "            _, predictions = torch.max(logits, dim=1)\n",
        "            num_correct += (predictions == labels).sum().item()\n",
        "            num_samples += len(inputs)\n",
        "\n",
        "    # Compute the average loss and accuracy\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = num_correct / num_samples\n",
        "\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "nll9dmt8GrNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model #1: Initializing ResNet"
      ],
      "metadata": {
        "id": "gerqlNxHauOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "                        nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n",
        "                        nn.BatchNorm2d(out_channels),\n",
        "                        nn.ReLU())\n",
        "        self.conv2 = nn.Sequential(\n",
        "                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
        "                        nn.BatchNorm2d(out_channels))\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU()\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        if self.downsample:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet50(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes = 7):\n",
        "        super(ResNet50, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Sequential(\n",
        "                        nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3),\n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.ReLU())\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
        "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
        "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
        "        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
        "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "        normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010],)\n",
        "        self.data_transform = transforms.Compose([transforms.Resize((224,224)),transforms.ToTensor(),normalize,])\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(planes),\n",
        "            )\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer0(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "mslqtPdQ7-6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model #2: Initializing AlexNet"
      ],
      "metadata": {
        "id": "vce0BgH0CkCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU())\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU())\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(9216, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc2= nn.Sequential(\n",
        "            nn.Linear(4096, num_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "w-R2k8U4E-P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model #3: Initializing VGG16"
      ],
      "metadata": {
        "id": "jCcUWgGUCunP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG16(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU())\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU())\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.layer6 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.layer7 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer8 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer9 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer10 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.layer11 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer12 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.layer13 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(7*7*512, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc2= nn.Sequential(\n",
        "            nn.Linear(4096, num_classes))\n",
        "\n",
        "        normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010],)\n",
        "        self.data_transform = transforms.Compose([transforms.Resize((227,227)), transforms.ToTensor(), normalize,])\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = self.layer6(out)\n",
        "        out = self.layer7(out)\n",
        "        out = self.layer8(out)\n",
        "        out = self.layer9(out)\n",
        "        out = self.layer10(out)\n",
        "        out = self.layer11(out)\n",
        "        out = self.layer12(out)\n",
        "        out = self.layer13(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "_3eYVD3u0awb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Data"
      ],
      "metadata": {
        "id": "_uckZWRuLdRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resize = 227\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010],)\n",
        "data_transform = transforms.Compose([transforms.Resize((resize,resize)),transforms.ToTensor(),normalize,])\n",
        "\n",
        "batch_size = 32\n",
        "num_workers = 2\n",
        "\n",
        "train_data = SkinLesions(root_dir, \"ISIC2018_Task3_Training_GroundTruth.csv\", data_transform)\n",
        "val_data = SkinLesions(root_dir, \"ISIC2018_Task3_Validation_GroundTruth.csv\", data_transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_data, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
      ],
      "metadata": {
        "id": "DDXV3I2LDQUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluating the Model"
      ],
      "metadata": {
        "id": "z-4cryncOsK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training ResNet50"
      ],
      "metadata": {
        "id": "AwGFkp_eDagO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ResNet50 takes a different size input so we redefine the train_data here.\n",
        "\n",
        "resize_50 = 224\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010],)\n",
        "data_transform = transforms.Compose([transforms.Resize((resize_50,resize_50)),transforms.ToTensor(),normalize,])\n",
        "\n",
        "batch_size = 32\n",
        "num_workers = 2\n",
        "\n",
        "train_data = SkinLesions(root_dir, \"ISIC2018_Task3_Training_GroundTruth.csv\", data_transform)\n",
        "val_data = SkinLesions(root_dir, \"ISIC2018_Task3_Validation_GroundTruth.csv\", data_transform)\n",
        "\n",
        "train_loader2 = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "\n",
        "val_loader2 = torch.utils.data.DataLoader(\n",
        "    val_data, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
      ],
      "metadata": {
        "id": "uNpC-1WKFpGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet50(ResidualBlock, [3, 4, 6, 3]).to(device)\n",
        "\n",
        "learning_rate = 0.005\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n",
        "\n",
        "train(model, train_loader2, val_loader2, optimizer, criterion, device, num_epochs= 5)"
      ],
      "metadata": {
        "id": "feUfo_2o8sJ6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "071af369-29c2-409b-81ee-a262e6a4411d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5:  69%|██████▉   | 22/32 [00:08<00:03,  2.63it/s, loss=0.967]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-e0c80794bc8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-409c2896295e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Use tqdm to display a progress bar during training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'Epoch {epoch + 1}/{num_epochs}'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                 \u001b[0;31m# Move inputs and labels to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training AlexNet"
      ],
      "metadata": {
        "id": "VGBrWF6EDlS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, resize = AlexNet(), 227\n",
        "\n",
        "\n",
        "learning_rate = 0.005\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n",
        "\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs= 5)"
      ],
      "metadata": {
        "id": "wn4RejpIDodv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51c42dd6-8ef7-42a4-9f1f-33de26122322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 32/32 [00:11<00:00,  2.77it/s, loss=0.886]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.1761, Accuracy = 0.5959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 32/32 [00:11<00:00,  2.90it/s, loss=0.835]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.0470, Accuracy = 0.6839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 32/32 [00:11<00:00,  2.86it/s, loss=0.251]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.0420, Accuracy = 0.6373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 32/32 [00:13<00:00,  2.32it/s, loss=0.231]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.0308, Accuracy = 0.6788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 32/32 [00:11<00:00,  2.76it/s, loss=0.554]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.0008, Accuracy = 0.6632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training VGG16"
      ],
      "metadata": {
        "id": "ll2M6arZDudG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, resize = VGG16()\n",
        "\n",
        "learning_rate = 0.005\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n",
        "\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=20)"
      ],
      "metadata": {
        "id": "W2IFAFKrDtnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 1: Varying the amount of input data given to the model to train on\n"
      ],
      "metadata": {
        "id": "Q6FShkLnE4vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sizes = [400, 2000, 5000]"
      ],
      "metadata": {
        "id": "viWTuyGBFGEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in test_sizes:\n",
        "  resize = 224\n",
        "\n",
        "  normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010],)\n",
        "  data_transform = transforms.Compose([transforms.Resize((resize,resize)),transforms.ToTensor(),normalize,])\n",
        "\n",
        "  batch_size = 32\n",
        "  num_workers = 2\n",
        "\n",
        "  train_data = SkinLesions(root_dir, \"ISIC2018_Task3_Training_GroundTruth.csv\", data_transform, size = i)\n",
        "  val_data = SkinLesions(root_dir, \"ISIC2018_Task3_Validation_GroundTruth.csv\", data_transform)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(\n",
        "      val_data, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "  model = ResNet50(ResidualBlock, [3, 4, 6, 3]).to(device)\n",
        "\n",
        "  learning_rate = 0.005\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n",
        "\n",
        "  print('Training model on n = ', i, ' data points for ResNet50')\n",
        "  train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs= 5)"
      ],
      "metadata": {
        "id": "IIGlANJoF_ra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfcaef7b-f134-4fc8-d20d-87e8c74a3416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model on n =  400  data points for ResNet50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 13/13 [00:05<00:00,  2.42it/s, loss=0.603]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.4018, Accuracy = 0.6373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 13/13 [00:06<00:00,  2.14it/s, loss=0.986]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 2.7033, Accuracy = 0.6321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 13/13 [00:04<00:00,  2.69it/s, loss=1.18]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.5797, Accuracy = 0.5907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 13/13 [00:05<00:00,  2.24it/s, loss=1.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.1794, Accuracy = 0.6373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 13/13 [00:05<00:00,  2.47it/s, loss=0.988]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.2329, Accuracy = 0.5337\n",
            "Training model on n =  2000  data points for ResNet50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 63/63 [00:25<00:00,  2.47it/s, loss=1.35]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.3522, Accuracy = 0.6010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 63/63 [00:25<00:00,  2.44it/s, loss=0.752]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.9895, Accuracy = 0.6580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 63/63 [00:25<00:00,  2.48it/s, loss=0.528]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.1362, Accuracy = 0.6425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 63/63 [00:25<00:00,  2.46it/s, loss=1.64]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.0405, Accuracy = 0.6891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 63/63 [00:25<00:00,  2.51it/s, loss=0.969]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.9573, Accuracy = 0.6632\n",
            "Training model on n =  5000  data points for ResNet50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 157/157 [01:04<00:00,  2.42it/s, loss=0.367]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.9979, Accuracy = 0.6580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 157/157 [01:09<00:00,  2.27it/s, loss=0.903]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.4421, Accuracy = 0.6528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 157/157 [01:03<00:00,  2.47it/s, loss=0.963]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.8625, Accuracy = 0.6684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 157/157 [01:02<00:00,  2.50it/s, loss=0.591]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.8857, Accuracy = 0.6839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 157/157 [01:05<00:00,  2.38it/s, loss=0.254]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.7996, Accuracy = 0.7150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in test_sizes:\n",
        "  resize = 227\n",
        "\n",
        "  normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010],)\n",
        "  data_transform = transforms.Compose([transforms.Resize((resize,resize)),transforms.ToTensor(),normalize,])\n",
        "\n",
        "  batch_size = 32\n",
        "  num_workers = 2\n",
        "\n",
        "  train_data = SkinLesions(root_dir, \"ISIC2018_Task3_Training_GroundTruth.csv\", data_transform, size = i)\n",
        "  val_data = SkinLesions(root_dir, \"ISIC2018_Task3_Validation_GroundTruth.csv\", data_transform)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(\n",
        "      val_data, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "  model = AlexNet()\n",
        "\n",
        "  learning_rate = 0.005\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n",
        "\n",
        "  print('Training model on n = ', i, ' data points for AlexNet')\n",
        "  train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs= 5)"
      ],
      "metadata": {
        "id": "iPAmGNITJQM_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f80374f-1f38-4011-940a-43251db17694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model on n =  400  data points for AlexNet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 13/13 [00:04<00:00,  2.82it/s, loss=1.07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.3079, Accuracy = 0.6373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 13/13 [00:05<00:00,  2.45it/s, loss=1.23]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.2412, Accuracy = 0.5907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 13/13 [00:04<00:00,  2.96it/s, loss=0.578]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.2246, Accuracy = 0.6010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 13/13 [00:04<00:00,  2.96it/s, loss=1.15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.0769, Accuracy = 0.6010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 13/13 [00:05<00:00,  2.28it/s, loss=0.827]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.2048, Accuracy = 0.6580\n",
            "Training model on n =  2000  data points for AlexNet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 63/63 [00:23<00:00,  2.72it/s, loss=0.697]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.3248, Accuracy = 0.6166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 63/63 [00:23<00:00,  2.66it/s, loss=0.947]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.1973, Accuracy = 0.6839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 63/63 [00:26<00:00,  2.34it/s, loss=1.64]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.9258, Accuracy = 0.7047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 63/63 [00:27<00:00,  2.26it/s, loss=0.602]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.1020, Accuracy = 0.6269\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 63/63 [00:23<00:00,  2.65it/s, loss=1.36]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.9364, Accuracy = 0.6891\n",
            "Training model on n =  5000  data points for AlexNet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 157/157 [00:56<00:00,  2.76it/s, loss=0.756]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.9566, Accuracy = 0.6995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 157/157 [00:58<00:00,  2.67it/s, loss=1.32]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.8823, Accuracy = 0.7150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 157/157 [00:57<00:00,  2.72it/s, loss=0.84]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.8177, Accuracy = 0.7098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 157/157 [00:57<00:00,  2.75it/s, loss=0.388]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.7628, Accuracy = 0.6891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 157/157 [00:57<00:00,  2.71it/s, loss=0.541]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.8204, Accuracy = 0.6995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in test_sizes:\n",
        "  resize = 227\n",
        "\n",
        "  normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010],)\n",
        "  data_transform = transforms.Compose([transforms.Resize((resize,resize)),transforms.ToTensor(),normalize,])\n",
        "\n",
        "  batch_size = 32\n",
        "  num_workers = 2\n",
        "\n",
        "  train_data = SkinLesions(root_dir, \"ISIC2018_Task3_Training_GroundTruth.csv\", data_transform, size = i)\n",
        "  val_data = SkinLesions(root_dir, \"ISIC2018_Task3_Validation_GroundTruth.csv\", data_transform)\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(\n",
        "      val_data, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "  model = VGG16()\n",
        "\n",
        "  learning_rate = 0.005\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n",
        "\n",
        "  print('Training model on n = ', i, ' data points for VGG16')\n",
        "  train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs= 5)"
      ],
      "metadata": {
        "id": "F7HZMZnHI81m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8f51b19-d1cc-4781-aefb-8807d7fd53ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model on n =  400  data points for VGG16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 13/13 [00:08<00:00,  1.51it/s, loss=0.46]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.6486, Accuracy = 0.6373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 13/13 [00:08<00:00,  1.61it/s, loss=1.03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.1453, Accuracy = 0.6114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 13/13 [00:08<00:00,  1.60it/s, loss=0.869]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.3805, Accuracy = 0.6010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 13/13 [00:08<00:00,  1.58it/s, loss=0.756]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.3499, Accuracy = 0.6062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 13/13 [00:08<00:00,  1.58it/s, loss=1.43]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.1751, Accuracy = 0.6632\n",
            "Training model on n =  2000  data points for VGG16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 63/63 [00:37<00:00,  1.68it/s, loss=1.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.3240, Accuracy = 0.4819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 63/63 [00:37<00:00,  1.69it/s, loss=0.609]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.0839, Accuracy = 0.6528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 63/63 [00:37<00:00,  1.69it/s, loss=1.02]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.2013, Accuracy = 0.6995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 63/63 [00:37<00:00,  1.70it/s, loss=0.774]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.1288, Accuracy = 0.6788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 63/63 [00:37<00:00,  1.69it/s, loss=0.401]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.9779, Accuracy = 0.6995\n",
            "Training model on n =  5000  data points for VGG16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 157/157 [01:33<00:00,  1.68it/s, loss=1.09]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.0422, Accuracy = 0.7047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 157/157 [01:34<00:00,  1.67it/s, loss=0.778]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.9636, Accuracy = 0.6632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 157/157 [01:33<00:00,  1.68it/s, loss=2.23]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.9715, Accuracy = 0.6995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 157/157 [01:33<00:00,  1.68it/s, loss=0.536]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.8610, Accuracy = 0.7254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 157/157 [01:36<00:00,  1.64it/s, loss=0.436]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.8687, Accuracy = 0.7461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment 2: Feature extraction and layer-specific tuning using pre-loaded models"
      ],
      "metadata": {
        "id": "46I0Ve4yGOWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resize = 227\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010],)\n",
        "data_transform = transforms.Compose([transforms.Resize((resize,resize)),transforms.ToTensor(),normalize,])\n",
        "\n",
        "batch_size = 32\n",
        "num_workers = 2\n",
        "\n",
        "train_data = SkinLesions(root_dir, \"ISIC2018_Task3_Training_GroundTruth.csv\", data_transform)\n",
        "val_data = SkinLesions(root_dir, \"ISIC2018_Task3_Validation_GroundTruth.csv\", data_transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_data, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
      ],
      "metadata": {
        "id": "Xf8MpIBARJsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resize = 224\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010],)\n",
        "data_transform = transforms.Compose([transforms.Resize((resize,resize)),transforms.ToTensor(),normalize,])\n",
        "\n",
        "batch_size = 32\n",
        "num_workers = 2\n",
        "\n",
        "train_data = SkinLesions(root_dir, \"ISIC2018_Task3_Training_GroundTruth.csv\", data_transform)\n",
        "val_data = SkinLesions(root_dir, \"ISIC2018_Task3_Validation_GroundTruth.csv\", data_transform)\n",
        "\n",
        "train_loader2 = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "\n",
        "val_loader2 = torch.utils.data.DataLoader(\n",
        "    val_data, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
      ],
      "metadata": {
        "id": "ywtntGlKRKI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using built in ResNet50"
      ],
      "metadata": {
        "id": "0zHZ7V0LNG57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "class Resnet(nn.Module):\n",
        "    def __init__(self, mode='finetune', pretrained=True):\n",
        "        super().__init__()\n",
        "        self.resnet = models.resnet50(pretrained = pretrained)\n",
        "        num_features = self.resnet.fc.in_features\n",
        "\n",
        "        if mode == 'linear':\n",
        "          for name, module in self.resnet.named_parameters():\n",
        "            module.requires_grad = False\n",
        "          self.resnet.fc = torch.nn.Linear(num_features, 7)\n",
        "\n",
        "        if mode == 'finetune':\n",
        "          self.resnet.fc = torch.nn.Linear(num_features, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.resnet.forward(x)\n",
        "      return x\n",
        "\n",
        "    def to(self,device):\n",
        "        return self.resnet.to(device=device)"
      ],
      "metadata": {
        "id": "O82Ai8OsLQAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Resnet(mode='linear',pretrained=True)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train(model, train_loader2, val_loader2, optimizer, criterion, device, num_epochs = 5)"
      ],
      "metadata": {
        "id": "5Jk9PT6OM9GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_layer = []\n",
        "layers_1_2 = []\n",
        "other_layers  = []\n",
        "\n",
        "model = Resnet(mode='finetune',pretrained=True)\n",
        "\n",
        "for name, module in model.named_parameters():\n",
        "  if 'fc' in name:\n",
        "    classifier_layer.append(module)\n",
        "  elif 'layer1' in name or 'layer2' in name:\n",
        "    layers_1_2.append(module)\n",
        "  else:\n",
        "    other_layers.append(module)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.SGD([\n",
        "                {'params': other_layers},\n",
        "                {'params': classifier_layer, 'lr': .01},\n",
        "                {'params': layers_1_2, 'lr': 0}\n",
        "            ], lr=0.001, momentum=0.9)\n",
        "\n",
        "train(model, train_loader2, val_loader2, optimizer, criterion, device, num_epochs = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t94v7yQGMb0H",
        "outputId": "ca615b6f-1dec-4908-9b5f-b1af318903ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 313/313 [02:36<00:00,  2.00it/s, loss=0.559]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.4656, Accuracy = 0.8342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 313/313 [02:26<00:00,  2.13it/s, loss=0.355]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.0347, Accuracy = 0.8446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 313/313 [02:26<00:00,  2.14it/s, loss=0.416]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.4418, Accuracy = 0.8756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 313/313 [02:27<00:00,  2.13it/s, loss=0.433]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.5869, Accuracy = 0.8394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 313/313 [02:27<00:00,  2.12it/s, loss=0.0724]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.5301, Accuracy = 0.8497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using built in Alexnet"
      ],
      "metadata": {
        "id": "5ndE4YmqQzvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, mode='finetune', pretrained=True):\n",
        "        super().__init__()\n",
        "        self.alexnet = models.alexnet(pretrained = pretrained)\n",
        "        num_features = self.alexnet.classifier[6].in_features\n",
        "\n",
        "        if mode == 'linear':\n",
        "          for name, module in self.alexnet.named_parameters():\n",
        "            module.requires_grad = False\n",
        "          self.alexnet.classifier[6] = torch.nn.Linear(num_features, 7)\n",
        "\n",
        "        if mode == 'finetune':\n",
        "          self.alexnet.classifier[6] = torch.nn.Linear(num_features, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.alexnet.forward(x)\n",
        "      return x\n",
        "\n",
        "    def to(self,device):\n",
        "        return self.alexnet.to(device=device)"
      ],
      "metadata": {
        "id": "7kjhouzwQ2nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AlexNet(mode='linear',pretrained=True)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fljvOK0sQ1hV",
        "outputId": "5b4cb1ce-075c-490f-b581-7a7639293bac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Epoch 1/5: 100%|██████████| 313/313 [01:53<00:00,  2.75it/s, loss=1.28]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.4442, Accuracy = 0.6632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 313/313 [01:52<00:00,  2.77it/s, loss=2.43]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.6479, Accuracy = 0.6736\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 313/313 [01:53<00:00,  2.77it/s, loss=1.79]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 2.2089, Accuracy = 0.6839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 313/313 [01:52<00:00,  2.78it/s, loss=2.56]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.1424, Accuracy = 0.6995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 313/313 [01:52<00:00,  2.79it/s, loss=2.31]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.9629, Accuracy = 0.8031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_layer = []\n",
        "layers_1_2 = []\n",
        "other_layers  = []\n",
        "\n",
        "model = AlexNet(mode='finetune',pretrained=True)\n",
        "\n",
        "for name, module in model.named_parameters():\n",
        "  if 'Linear' in name:\n",
        "    classifier_layer.append(module)\n",
        "  elif 'layer1' in name or 'layer2' in name:\n",
        "    layers_1_2.append(module)\n",
        "  else:\n",
        "    other_layers.append(module)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.SGD([\n",
        "                {'params': other_layers},\n",
        "                {'params': classifier_layer, 'lr': .01},\n",
        "                {'params': layers_1_2, 'lr': 0}\n",
        "            ], lr=0.001, momentum=0.9)\n",
        "\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBnzgCeBM0-F",
        "outputId": "6b01fc87-0d9e-4be2-ffd7-a28199363f1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 313/313 [01:55<00:00,  2.71it/s, loss=0.707]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.6725, Accuracy = 0.7772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 313/313 [01:56<00:00,  2.68it/s, loss=0.474]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.4936, Accuracy = 0.8135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 313/313 [01:56<00:00,  2.69it/s, loss=0.649]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.4992, Accuracy = 0.8238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 313/313 [01:56<00:00,  2.68it/s, loss=0.267]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.4658, Accuracy = 0.8290\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 313/313 [01:55<00:00,  2.70it/s, loss=0.353]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.4570, Accuracy = 0.8653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using built in VGG16"
      ],
      "metadata": {
        "id": "DHGq7jnFQFnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vgg16(nn.Module):\n",
        "    def __init__(self, mode='finetune', pretrained=True):\n",
        "        super().__init__()\n",
        "        self.vgg16 = models.vgg16(pretrained = pretrained)\n",
        "        num_features = self.vgg16.classifier[6].in_features\n",
        "\n",
        "        if mode == 'linear':\n",
        "          for name, module in self.vgg16.named_parameters():\n",
        "            module.requires_grad = False\n",
        "          self.vgg16.classifier[6] = torch.nn.Linear(num_features, 7)\n",
        "\n",
        "        if mode == 'finetune':\n",
        "          self.vgg16.classifier[6] = torch.nn.Linear(num_features, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.vgg16.forward(x)\n",
        "      return x\n",
        "\n",
        "    def to(self,device):\n",
        "        return self.vgg16.to(device=device)"
      ],
      "metadata": {
        "id": "3lUfXHorQK8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Vgg16(mode='linear', pretrained = True)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.005, momentum = 0.9)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LstJv1j_Qakw",
        "outputId": "9d2aa4a6-f6e0-40c7-c8f7-54f0bfc04a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Epoch 1/5: 100%|██████████| 313/313 [02:17<00:00,  2.28it/s, loss=1.34]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.1109, Accuracy = 0.6891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 313/313 [02:09<00:00,  2.42it/s, loss=1.83]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.1856, Accuracy = 0.6580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 313/313 [02:06<00:00,  2.47it/s, loss=2.13]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.9437, Accuracy = 0.6632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 313/313 [02:06<00:00,  2.47it/s, loss=2.05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.0940, Accuracy = 0.7098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 313/313 [02:05<00:00,  2.49it/s, loss=1.15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.8711, Accuracy = 0.6943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_layer = []\n",
        "layers_1_2 = []\n",
        "other_layers  = []\n",
        "\n",
        "model = Vgg16(mode='finetune',pretrained=True)\n",
        "\n",
        "for name, module in model.named_parameters():\n",
        "  if 'Linear' in name:\n",
        "    classifier_layer.append(module)\n",
        "  elif 'layer1' in name or 'layer2' in name:\n",
        "    layers_1_2.append(module)\n",
        "  else:\n",
        "    other_layers.append(module)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.SGD([\n",
        "                {'params': other_layers},\n",
        "                {'params': classifier_layer, 'lr': .01},\n",
        "                {'params': layers_1_2, 'lr': 0}\n",
        "            ], lr=0.001, momentum=0.9)\n",
        "\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Qc5VVKQWGlj",
        "outputId": "b044cbe6-d423-49e7-89e0-07a7d4fd18b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Epoch 1/5: 100%|██████████| 313/313 [02:46<00:00,  1.88it/s, loss=0.885]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.7101, Accuracy = 0.7772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 313/313 [02:44<00:00,  1.90it/s, loss=0.721]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.6000, Accuracy = 0.7565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 313/313 [02:43<00:00,  1.91it/s, loss=0.548]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.5593, Accuracy = 0.8135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 313/313 [02:43<00:00,  1.91it/s, loss=0.506]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.6708, Accuracy = 0.7720\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 313/313 [02:43<00:00,  1.91it/s, loss=0.415]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.5383, Accuracy = 0.8238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 3: Look at specific worse-performing features\n"
      ],
      "metadata": {
        "id": "I6_ZIAY8Rm9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs):\n",
        "    # Place model on device\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        # Use tqdm to display a progress bar during training\n",
        "        with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{num_epochs}') as pbar:\n",
        "            for inputs, labels in train_loader:\n",
        "                # Move inputs and labels to device\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # Zero out gradients\n",
        "                optimizer.zero_grad()\n",
        "                # Compute the logits and loss\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, labels)\n",
        "                # Backpropagate the loss\n",
        "                loss.backward()\n",
        "                # Update the weights\n",
        "                optimizer.step()\n",
        "                # Update the progress bar\n",
        "                pbar.update(1)\n",
        "                pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Evaluate the model on the validation set\n",
        "        avg_loss, accuracy, label_loss_counter = evaluate(model, val_loader, criterion, device)\n",
        "        print(f'Validation set: Average loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}')\n",
        "    return label_loss_counter\n",
        "\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    label_loss_counter = {}\n",
        "\n",
        "    for i in range(8):\n",
        "      label_loss_counter[i] = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0.0\n",
        "        num_correct = 0\n",
        "        num_samples = 0\n",
        "\n",
        "        for inputs, labels in test_loader:\n",
        "            # Move inputs and labels to device\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Compute the logits and loss\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Compute the accuracy\n",
        "            _, predictions = torch.max(logits, dim=1)\n",
        "            num_correct += (predictions == labels).sum().item()\n",
        "            num_samples += len(inputs)\n",
        "\n",
        "            #print(labels)\n",
        "            for i in range(len(predictions)):\n",
        "              if predictions[i] != labels[i]:\n",
        "                prediction = int(predictions[i])\n",
        "                label_loss_counter[prediction] += 1\n",
        "\n",
        "    # Compute the average loss and accuracy\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = num_correct / num_samples\n",
        "\n",
        "    return avg_loss, accuracy, label_loss_counter"
      ],
      "metadata": {
        "id": "EDqYXduUS8vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resize = 224\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010],)\n",
        "data_transform = transforms.Compose([transforms.Resize((resize,resize)),transforms.ToTensor(),normalize,])\n",
        "\n",
        "batch_size = 32\n",
        "num_workers = 2\n",
        "\n",
        "train_data = SkinLesions(root_dir, \"ISIC2018_Task3_Training_GroundTruth.csv\", data_transform, size = 10000)\n",
        "val_data = SkinLesions(root_dir, \"ISIC2018_Task3_Validation_GroundTruth.csv\", data_transform)\n",
        "\n",
        "train_loader2 = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "\n",
        "val_loader2 = torch.utils.data.DataLoader(\n",
        "    val_data, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
      ],
      "metadata": {
        "id": "P5NpIQinWuPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_layer = []\n",
        "layers_1_2 = []\n",
        "other_layers  = []\n",
        "\n",
        "model = Resnet(mode='finetune',pretrained=True)\n",
        "\n",
        "for name, module in model.named_parameters():\n",
        "  if 'fc' in name:\n",
        "    classifier_layer.append(module)\n",
        "  elif 'layer1' in name or 'layer2' in name:\n",
        "    layers_1_2.append(module)\n",
        "  else:\n",
        "    other_layers.append(module)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.SGD([\n",
        "                {'params': other_layers},\n",
        "                {'params': classifier_layer, 'lr': .01},\n",
        "                {'params': layers_1_2, 'lr': 0}\n",
        "            ], lr=0.001, momentum=0.9)\n",
        "\n",
        "label_loss_counter_resnet = train(model, train_loader2, val_loader2, optimizer, criterion, device, num_epochs = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJ4rAVoXU-Pb",
        "outputId": "bd37c5aa-7f90-46fe-c9bf-081005639ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2: 100%|██████████| 313/313 [02:34<00:00,  2.03it/s, loss=0.915]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.5410, Accuracy = 0.8083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2: 100%|██████████| 313/313 [02:23<00:00,  2.18it/s, loss=0.323]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.4727, Accuracy = 0.8446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_loss_counter_resnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgoagTcQiMP_",
        "outputId": "01c51d12-7b4e-4ffd-9a45-e21eccd29078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 4, 1: 18, 2: 0, 3: 2, 4: 6, 5: 0, 6: 0, 7: 0}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_layer = []\n",
        "layers_1_2 = []\n",
        "other_layers  = []\n",
        "\n",
        "model = AlexNet(mode='finetune',pretrained=True)\n",
        "\n",
        "for name, module in model.named_parameters():\n",
        "  if 'Linear' in name:\n",
        "    classifier_layer.append(module)\n",
        "  elif 'layer1' in name or 'layer2' in name:\n",
        "    layers_1_2.append(module)\n",
        "  else:\n",
        "    other_layers.append(module)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.SGD([\n",
        "                {'params': other_layers},\n",
        "                {'params': classifier_layer, 'lr': .01},\n",
        "                {'params': layers_1_2, 'lr': 0}\n",
        "            ], lr=0.001, momentum=0.9)\n",
        "\n",
        "label_loss_counter_alex = train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIEduXFBa5PZ",
        "outputId": "d31c79fc-2699-4249-eb4e-157ed663295c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Epoch 1/2: 100%|██████████| 313/313 [02:00<00:00,  2.61it/s, loss=0.44]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.5979, Accuracy = 0.8083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2: 100%|██████████| 313/313 [01:53<00:00,  2.75it/s, loss=1.16]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.5016, Accuracy = 0.8394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_loss_counter_alex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGEAvlzYiRjE",
        "outputId": "663ad215-1162-4053-b204-c0719726e29d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 3, 1: 8, 2: 6, 3: 1, 4: 13, 5: 0, 6: 0, 7: 0}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_layer = []\n",
        "layers_1_2 = []\n",
        "other_layers  = []\n",
        "\n",
        "model = Vgg16(mode='finetune',pretrained=True)\n",
        "\n",
        "for name, module in model.named_parameters():\n",
        "  if 'Linear' in name:\n",
        "    classifier_layer.append(module)\n",
        "  elif 'layer1' in name or 'layer2' in name:\n",
        "    layers_1_2.append(module)\n",
        "  else:\n",
        "    other_layers.append(module)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.SGD([\n",
        "                {'params': other_layers},\n",
        "                {'params': classifier_layer, 'lr': .01},\n",
        "                {'params': layers_1_2, 'lr': 0}\n",
        "            ], lr=0.001, momentum=0.9)\n",
        "\n",
        "label_loss_counter_Vgg = train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQpI74ZSaoZ-",
        "outputId": "5842d126-fde1-492f-b17b-ca5a3df727d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Epoch 1/2: 100%|██████████| 313/313 [02:37<00:00,  1.99it/s, loss=0.628]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.6127, Accuracy = 0.7772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2: 100%|██████████| 313/313 [02:36<00:00,  2.00it/s, loss=0.84]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 0.6300, Accuracy = 0.7927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_loss_counter_Vgg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLU50TXUiXf2",
        "outputId": "a66b37c8-9fe0-416d-fa58-0209c8408abc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 16, 1: 8, 2: 5, 3: 2, 4: 8, 5: 0, 6: 1, 7: 0}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment 4: Model Mean and Performance Visualization\n",
        "\n",
        "First create tensor with only image values."
      ],
      "metadata": {
        "id": "sQo8FZxWVfcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_only = torch.zeros(len(train_data), 3, 227, 227)\n",
        "train_img_only.shape\n",
        "\n",
        "for i in range(len(train_data)):\n",
        "  train_img_only[i] = train_data[i][0]"
      ],
      "metadata": {
        "id": "ClYMg5VSVjmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Pred and Label Arrays after passing images through model to classify."
      ],
      "metadata": {
        "id": "F_46ALhLaihB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = []\n",
        "count = 0\n",
        "for img in train_img_only:\n",
        "  out = model.forward(img.cuda().unsqueeze(0))\n",
        "  out.cuda()\n",
        "  _, prediction = torch.max(out, dim=1)\n",
        "  prediction.cuda()\n",
        "  prediction = prediction.cpu().numpy()[0]\n",
        "  pred.append(prediction)\n",
        "  count += 1\n"
      ],
      "metadata": {
        "id": "HDCexmVG7zZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = np.array(pred)\n",
        "labels = []\n",
        "for i in range(len(train_data)):\n",
        "  label = train_data[i][1]\n",
        "  labels.append(label)"
      ],
      "metadata": {
        "id": "2P3ZDLEj84qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Counter to get an idea of how classifier predicted outputs"
      ],
      "metadata": {
        "id": "1Qt5q75va-c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "print(Counter(pred))\n",
        "Counter(labels)"
      ],
      "metadata": {
        "id": "YohEXsoKahRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Index Arrays"
      ],
      "metadata": {
        "id": "5In5hqPBbdeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_directory = {0: 'MEL', 1: 'NV', 2: 'BCC', 3: 'AKIEC', 4: 'BKL', 5: 'DF', 6: 'VASC'}\n",
        "melanoma_label = [1 if i == 6 else 0 for i in labels]\n",
        "melanoma_label_indices = []\n",
        "for i, a in enumerate(melanoma_label):\n",
        "  if a == 1:\n",
        "    melanoma_label_indices.append(i)\n",
        "\n",
        "label_directory = {0: 'MEL', 1: 'NV', 2: 'BCC', 3: 'AKIEC', 4: 'BKL', 5: 'DF', 6: 'VASC'}\n",
        "melanoma_pred = [1 if i == 6 else 0 for i in pred]\n",
        "melanoma_pred_indices = []\n",
        "for i, a in enumerate(melanoma_pred):\n",
        "  if a == 1:\n",
        "    melanoma_pred_indices.append(i)"
      ],
      "metadata": {
        "id": "rluyVfGZanK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use index arrays to calculate ground truth and predicted image arrays, and use those to compute mean image"
      ],
      "metadata": {
        "id": "hxIGj082bP45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mel_truth_img_only = train_img_only[melanoma_label_indices]\n",
        "mel_pred_img_only = train_img_only[melanoma_pred_indices]\n",
        "\n",
        "sum = torch.zeros(3, 227, 227)\n",
        "count = 0\n",
        "for img in train_img_only:\n",
        "  sum += img\n",
        "total_average = sum / len(train_img_only)\n",
        "sum = 0\n",
        "for img in mel_truth_img_only:\n",
        "  sum += img\n",
        "mel_truth_average = sum / len(mel_truth_img_only)\n",
        "sum = 0\n",
        "for img in mel_pred_img_only:\n",
        "  sum += img\n",
        "mel_pred_average = sum / len(mel_pred_img_only)\n",
        ""
      ],
      "metadata": {
        "id": "M1tneLe_a0j6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show images"
      ],
      "metadata": {
        "id": "bN49pT7pbQ_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "plt.imshow(  mel_truth_average.permute(1, 2, 0)  )\n",
        "plt.imshow(  mel_pred_average.permute(1, 2, 0)  )"
      ],
      "metadata": {
        "id": "nU71mqL3a4ih"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
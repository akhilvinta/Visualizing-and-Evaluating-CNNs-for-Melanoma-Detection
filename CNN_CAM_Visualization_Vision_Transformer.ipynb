{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKJx4TLuF1q5"
      },
      "source": [
        "Download the MiniPlaces dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MNtgNN3FF1q6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2466e24a-9e9c-40de-f3bf-d4862b53db2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Downloading...\n",
            "From (uriginal): https://drive.google.com/uc?id=16GYHdSWS3iMYwMPv5FpeDZN2rH7PR0F2\n",
            "From (redirected): https://drive.google.com/uc?id=16GYHdSWS3iMYwMPv5FpeDZN2rH7PR0F2&confirm=t&uuid=48139d6e-bc72-4462-9b67-e0a36fb19c21\n",
            "To: /content/data.tar.gz\n",
            "100% 460M/460M [00:04<00:00, 105MB/s]\n"
          ]
        }
      ],
      "source": [
        "!pip install einops\n",
        "# Downloading this file takes about a few seconds.\n",
        "# Download the tar.gz file from google drive using its file ID.\n",
        "!pip3 install --upgrade gdown --quiet\n",
        "!gdown 16GYHdSWS3iMYwMPv5FpeDZN2rH7PR0F2 # this is the file ID of miniplaces dataset\n",
        "# !gdown 1CyIQOJienhNITwGcQ9h-nv8z6GOjV2HX\n",
        "# back-up commands (try the following it previous file id is overload)\n",
        "# !gdown 1CyIQOJienhNITwGcQ9h-nv8z6GOjV2HX\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tB8tvZhIF1q9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "from tqdm import tqdm\n",
        "import urllib.request\n",
        "\n",
        "def setup(file_link_dict={},\n",
        "          folder_name='Assignment3'):\n",
        "  CS188_path = './'\n",
        "  os.makedirs(os.path.join(CS188_path, 'Assignment3', 'data'), exist_ok=True)\n",
        "  root_dir = os.path.join(CS188_path, 'Assignment3')\n",
        "  tar = tarfile.open(\"data.tar.gz\", \"r:gz\")\n",
        "  total_size = sum(f.size for f in tar.getmembers())\n",
        "  with tqdm(total=total_size, unit=\"B\", unit_scale=True, desc=\"Extracting tar.gz file\") as pbar:\n",
        "      for member in tar.getmembers():\n",
        "          tar.extract(member, os.path.join(root_dir, 'data'))\n",
        "          pbar.update(member.size)\n",
        "  tar.close()\n",
        "  for file_name, file_link in file_link_dict.items():\n",
        "      print(f'Downloding {file_name}.txt from {file_link}')\n",
        "      urllib.request.urlretrieve(file_link, f'{root_dir}/data/{file_name}.txt')\n",
        "  return root_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wx-2pvciF1q-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "051afc35-f6d2-4cb7-cf1d-7f501131413d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting tar.gz file: 100%|██████████| 566M/566M [00:22<00:00, 25.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloding train.txt from https://raw.githubusercontent.com/CSAILVision/miniplaces/master/data/train.txt\n",
            "Downloding val.txt from https://raw.githubusercontent.com/CSAILVision/miniplaces/master/data/val.txt\n"
          ]
        }
      ],
      "source": [
        "val_url = 'https://raw.githubusercontent.com/CSAILVision/miniplaces/master/data/val.txt'\n",
        "train_url = 'https://raw.githubusercontent.com/CSAILVision/miniplaces/master/data/train.txt'\n",
        "root_dir = setup(\n",
        "    file_link_dict={'train':train_url, 'val':val_url},\n",
        "    folder_name='Assignment3')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4MyTph9F1q_"
      },
      "source": [
        "### Define the data transform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1v9w2JugF1rA"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Define data transformation\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize(128),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXrmPXGEF1rB"
      },
      "source": [
        "### Define the dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KoHHge5YF1rB"
      },
      "outputs": [],
      "source": [
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class MiniPlaces(Dataset):\n",
        "    def __init__(self, root_dir, split, transform=None, label_dict=None):\n",
        "        \"\"\"\n",
        "        Initialize the MiniPlaces dataset with the root directory for the images,\n",
        "        the split (train/val/test), an optional data transformation,\n",
        "        and an optional label dictionary.\n",
        "\n",
        "        Args:\n",
        "            root_dir (str): Root directory for the MiniPlaces images.\n",
        "            split (str): Split to use ('train', 'val', or 'test').\n",
        "            transform (callable, optional): Optional data transformation to apply to the images.\n",
        "            label_dict (dict, optional): Optional dictionary mapping integer labels to class names.\n",
        "        \"\"\"\n",
        "        assert split in ['train', 'val', 'test']\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.filenames = []\n",
        "        self.labels = []\n",
        "        self.label_dict = label_dict if label_dict is not None else {}\n",
        "\n",
        "        if split == 'train':\n",
        "          f = open(root_dir + \"/train.txt\")\n",
        "          for line in f:\n",
        "            className = line.split(\"/\")[2]\n",
        "            classId = int(line.strip('\\n').split(\" \")[-1])\n",
        "            self.label_dict[classId] = className\n",
        "            self.filenames.append(line.split(\" \")[0])\n",
        "            self.labels.append(classId)\n",
        "\n",
        "        if split == 'val':\n",
        "          f = open(root_dir + \"/val.txt\")\n",
        "          for line in f:\n",
        "            filePath = line.split(' ')[0]\n",
        "            self.filenames.append(filePath)\n",
        "            classId = int(line.strip('\\n').split(\" \")[-1])\n",
        "            self.labels.append(classId)\n",
        "\n",
        "\n",
        "        if split == 'test':\n",
        "          directoryFiles = sorted(os.listdir(root_dir + \"/images/test\"))\n",
        "          for file in directoryFiles:\n",
        "            self.filenames.append(\"test/\" + file)\n",
        "            self.labels.append(0)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the number of images in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of images in the dataset.\n",
        "        \"\"\"\n",
        "        # Return the number of images in the dataset\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Return a single image and its corresponding label when given an index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the image to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Tuple containing the image and its label.\n",
        "        \"\"\"\n",
        "        image = None\n",
        "        label = None\n",
        "        # Load and preprocess image using self.root_dir,\n",
        "        # self.filenames[idx], and self.transform (if specified)\n",
        "        image = Image.open(self.root_dir + \"/images/\" + self.filenames[idx])\n",
        "        if self.transform:\n",
        "          image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_th2HTIF1rC"
      },
      "source": [
        "### Define the train method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kfX-uwDDF1rC"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs):\n",
        "    \"\"\"\n",
        "    Train the MLP classifier on the training set and evaluate it on the validation set every epoch.\n",
        "\n",
        "    Args:\n",
        "        model (MLP): MLP classifier to train.\n",
        "        train_loader (torch.utils.data.DataLoader): Data loader for the training set.\n",
        "        val_loader (torch.utils.data.DataLoader): Data loader for the validation set.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer to use for training.\n",
        "        criterion (callable): Loss function to use for training.\n",
        "        device (torch.device): Device to use for training.\n",
        "        num_epochs (int): Number of epochs to train the model.\n",
        "    \"\"\"\n",
        "    # Place model on device\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        # Use tqdm to display a progress bar during training\n",
        "        with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{num_epochs}') as pbar:\n",
        "            for inputs, labels in train_loader:\n",
        "                # Move inputs and labels to device\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero out gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Compute the logits and loss\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "                # Backpropagate the loss\n",
        "                loss.backward()\n",
        "\n",
        "                # Update the weights\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update the progress bar\n",
        "                pbar.update(1)\n",
        "                pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Evaluate the model on the validation set\n",
        "        avg_loss, accuracy = evaluate(model, val_loader, criterion, device)\n",
        "        print(f'Validation set: Average loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}')\n",
        "\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluate the MLP classifier on the test set.\n",
        "\n",
        "    Args:\n",
        "        model (MLP): MLP classifier to evaluate.\n",
        "        test_loader (torch.utils.data.DataLoader): Data loader for the test set.\n",
        "        criterion (callable): Loss function to use for evaluation.\n",
        "        device (torch.device): Device to use for evaluation.\n",
        "\n",
        "    Returns:\n",
        "        float: Average loss on the test set.\n",
        "        float: Accuracy on the test set.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0.0\n",
        "        num_correct = 0\n",
        "        num_samples = 0\n",
        "\n",
        "        for inputs, labels in test_loader:\n",
        "            # Move inputs and labels to device\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Compute the logits and loss\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Compute the accuracy\n",
        "            _, predictions = torch.max(logits, dim=1)\n",
        "            num_correct += (predictions == labels).sum().item()\n",
        "            num_samples += len(inputs)\n",
        "\n",
        "    # Compute the average loss and accuracy\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = num_correct / num_samples\n",
        "\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the KNN"
      ],
      "metadata": {
        "id": "BFVUfTT6Z6OT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "z2Z9TE_UF1rF"
      },
      "outputs": [],
      "source": [
        "def compute_distances_no_loops(x_train, x_test):\n",
        "  num_train = x_train.shape[0]\n",
        "  num_test = x_test.shape[0]\n",
        "  dists = x_train.new_zeros(num_train, num_test)\n",
        "\n",
        "  A = x_train.reshape(num_train,-1)\n",
        "  B = x_test.reshape(num_test,-1)\n",
        "  AB2 = A.mm(B.T)*2\n",
        "  dists = ((A**2).sum(dim = 1).reshape(-1,1) - AB2 + (B**2).sum(dim = 1).reshape(1,-1))**(1/2)\n",
        "  return dists\n",
        "\n",
        "def predict_labels(dists, y_train, k=1):\n",
        "  num_train, num_test = dists.shape\n",
        "  y_pred = torch.zeros(num_test, dtype=torch.int64)\n",
        "\n",
        "  values, indices = torch.topk(dists, k, dim=0, largest=False)\n",
        "  for i in range(indices.shape[1]):\n",
        "    _, idx = torch.max(y_train[indices[:,i]].bincount(), dim = 0)\n",
        "    y_pred[i] = idx\n",
        "  return indices, y_pred\n",
        "\n",
        "class KnnClassifier:\n",
        "  def __init__(self, x_train, y_train):\n",
        "    self.x_train = x_train\n",
        "    self.y_train = y_train\n",
        "\n",
        "  def predict(self, x_test, k=1):\n",
        "    y_test_pred = None\n",
        "\n",
        "    dists = compute_distances_no_loops(self.x_train, x_test)\n",
        "    _, y_test_pred =  predict_labels(dists, self.y_train, k)\n",
        "\n",
        "    return y_test_pred\n",
        "\n",
        "  def check_accuracy(self, x_test, y_test, k=1, quiet=False):\n",
        "    y_test_pred = self.predict(x_test, k=k)\n",
        "    num_samples = x_test.shape[0]\n",
        "    num_correct = (y_test == y_test_pred).sum().item()\n",
        "    accuracy = 100.0 * num_correct / num_samples\n",
        "    msg = (f'Got {num_correct} / {num_samples} correct; '\n",
        "           f'accuracy is {accuracy:.2f}%')\n",
        "    if not quiet:\n",
        "      print(msg)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Pob9aqlkF1rF"
      },
      "outputs": [],
      "source": [
        "# Also, seed everything for reproducibility\n",
        "# code from https://gist.github.com/ihoromi4/b681a9088f348942b01711f251e5f964#file-seed_everything-py\n",
        "def seed_everything(seed: int):\n",
        "    import random, os\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dMTQ7y7lF1rF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4ee8060-8763-4f6a-e38c-386e91c7cb53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda. Good to go!\n"
          ]
        }
      ],
      "source": [
        "# Define the device to use for training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device == torch.device('cuda'):\n",
        "    print(f'Using device: {device}. Good to go!')\n",
        "else:\n",
        "    print('Please set GPU via Edit -> Notebook Settings.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "y67XuOdxF1rG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "896204c9-8a64-444a-d369-993b72c4bd03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Aug 28 06:42:06 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8     9W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAgPy9WcR9X5"
      },
      "source": [
        "## ResNet\n",
        "\n",
        "In this question, you will learn how to use pretrained model and apply transfer learning on ResNet18."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbkkxjvMU67L"
      },
      "source": [
        "### Build the ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "exoAuQN5F1rU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torchvision.models as models\n",
        "class Resnet(nn.Module):\n",
        "    def __init__(self, mode='finetune',pretrained=True):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        use the resnet18 model from torchvision models. Remember to set pretrained as true\n",
        "\n",
        "        mode has three options:\n",
        "        1) features: to extract features only, we do not want the last fully connected layer of\n",
        "            resnet18. Use nn.Identity() to replace this layer.\n",
        "        2) linear: For this model, we want to freeze resnet18 features, then train a linear\n",
        "            classifier which takes the features before FC (again we do not want\n",
        "            resnet18 FC). And then write our own FC layer: which takes in the features and\n",
        "            output scores of size 100 (because we have 100 categories).\n",
        "            Because we want to freeze resnet18 features, we have to iterate through parameters()\n",
        "            of our model, and manually set some parameters to requires_grad = False\n",
        "            Or use other methods to freeze the features\n",
        "        3) finetune: Same as 2), except that we we do not need to freeze the features and\n",
        "           can finetune on the pretrained resnet model.\n",
        "        \"\"\"\n",
        "        self.resnet = None\n",
        "        self.resnet = models.resnet18(pretrained=pretrained)\n",
        "\n",
        "        if mode == 'feature':\n",
        "          self.resnet.fc = nn.Identity()\n",
        "        if mode == 'linear':\n",
        "          for param in self.resnet.parameters():\n",
        "                param.requires_grad = False\n",
        "          self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 100)\n",
        "        if mode == 'finetune':\n",
        "          for param in self.resnet.parameters():\n",
        "            param.requires_grad = True\n",
        "          self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 100)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "        return x\n",
        "\n",
        "    def to(self,device):\n",
        "        return self.resnet.to(device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA4FCBixCarC"
      },
      "source": [
        "### Feature comparison\n",
        "Recall that in Assignment1, we used color histogram features for KNN classification.\n",
        "In this assignment, we will use features from ResNet18 and compare the results.\n",
        "#### Color Features\n",
        "We will first used the color features as in Assignment1. You do not need to implement anything here. Copy your implementation of KNN in model.py, then run the codes below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DO6bqQHAwqOr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "562a9359-c5f3-44e8-b00a-9abf15a77d4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:01<00:00, 19.73it/s]\n"
          ]
        }
      ],
      "source": [
        "seed_everything(0)\n",
        "data_root = os.path.join(root_dir, 'data')\n",
        "train_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='train',\n",
        "    transform=data_transform)\n",
        "\n",
        "val_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='val',\n",
        "    transform=data_transform,\n",
        "    label_dict=train_dataset.label_dict)\n",
        "\n",
        "\n",
        "\n",
        "sub_sample = list(range(0, len(train_dataset), 50))\n",
        "print(len(sub_sample))\n",
        "training_data1 = torch.utils.data.Subset(train_dataset, sub_sample)\n",
        "training_loader1 = torch.utils.data.DataLoader(training_data1, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "sub_sample = list(range(0, len(val_dataset),16))\n",
        "val_dataset1 = torch.utils.data.Subset(val_dataset, sub_sample)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(val_dataset1, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "all_features = []\n",
        "all_labels = []\n",
        "\n",
        "for i, data in enumerate(tqdm((training_loader1))):\n",
        "    inputs, label = data\n",
        "\n",
        "    all_features.append(inputs)\n",
        "    all_labels.append(label)\n",
        "\n",
        "all_features = torch.cat(all_features, dim=0)\n",
        "all_labels = torch.cat(all_labels, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "a2_Ip8odx2nF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "035b6d5c-9cfe-4a32-b5bb-20581591d2e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:04<00:00,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total accuracy is 0.04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "\n",
        "total_acc = 0\n",
        "true_pos = 0\n",
        "total = 0\n",
        "knn = KnnClassifier(all_features, all_labels)\n",
        "for i, data in enumerate(tqdm((test_loader))):\n",
        "    inputs, label = data\n",
        "    acc = knn.check_accuracy(inputs, label, k=10, quiet=True)\n",
        "    true_pos += acc*inputs.shape[0]\n",
        "    total += inputs.shape[0]\n",
        "\n",
        "total_acc = true_pos / total\n",
        "print (\"total accuracy is %.2f\"%(total_acc/100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oMGIHr9_yiW"
      },
      "source": [
        "I got an accuracy of 4% using my own implementation. How about you?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w45nyWfWzcTJ"
      },
      "source": [
        "\n",
        "Implement the Resnet class above. First write the case where \"mode=feature\". This means that we want to discard the final FC layer of ResNet18."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8mbTFMl8zZKU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff4eac84-bb41-4163-ff93-6fb830e7bd88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 203MB/s]\n",
            "100%|██████████| 32/32 [00:10<00:00,  3.11it/s]\n"
          ]
        }
      ],
      "source": [
        "resnet18 = Resnet(mode='feature',pretrained=True)\n",
        "resnet18.to(device)\n",
        "\n",
        "\n",
        "all_features = []\n",
        "all_labels = []\n",
        "\n",
        "for i, data in enumerate(tqdm((training_loader1))):\n",
        "    inputs, label = data\n",
        "    inputs = inputs.cuda()\n",
        "    cnn_features = resnet18(inputs).detach().cpu()\n",
        "\n",
        "    all_features.append(cnn_features)\n",
        "    all_labels.append(label)\n",
        "\n",
        "all_features = torch.cat(all_features, dim=0)\n",
        "all_labels = torch.cat(all_labels, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Z5hRiOSWzuNi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27105a6a-f81a-4bfd-c294-32c1fd57cf98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:01<00:00,  2.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total accuracy is 0.10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "knn = KnnClassifier(all_features, all_labels)\n",
        "true_pos = 0\n",
        "total = 0\n",
        "for i, data in enumerate(tqdm((test_loader))):\n",
        "    inputs, label = data\n",
        "    inputs = inputs.cuda()\n",
        "    features = resnet18(inputs).detach().cpu()\n",
        "    acc = knn.check_accuracy(features, label, k=5, quiet=True)\n",
        "    true_pos += acc*inputs.shape[0]\n",
        "    total += inputs.shape[0]\n",
        "\n",
        "total_acc = true_pos / total\n",
        "print (\"total accuracy is %.2f\"%(total_acc/100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbAkvoGn_3GQ"
      },
      "source": [
        "I got an accuracy of 10% using my own implementation. How about you?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Zj1HRiBY1Ic3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqC58lscztxa"
      },
      "source": [
        "What's your opinion on using different features? Pros & Cons?\n",
        "\n",
        "Pros: This seems to be a very creative way to improve accuracy. Training on one model generating features with KNN is a very clever way to force more learning from the model. Cons: This method seems to take a significant amount of data to train. This model that is being imported is pre-trained on what I'm believing to be an enormous amount of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwHkysWXHaAs"
      },
      "source": [
        "### Pretrained ResNet features + Linear Classifier\n",
        "\n",
        "Then we implement the “linear” mode in Resnet class in model.py. Remember to freeze the features of ResNet. In this implementation we use linear classifier to do classification on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "tAjaC7YBHaAs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "4d40d6c5-5c7b-4095-da8d-de51243e16b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:   6%|▋         | 98/1563 [00:07<01:47, 13.61it/s, loss=3.73]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-bfc3b314b9ae>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-3cbf55430f53>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;31m# Compute the logits and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1599\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward_pre_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1601\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "# Define the model, optimizer, and criterion (loss_fn)\n",
        "model = Resnet(mode='linear',pretrained=True)\n",
        "\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=0.005,\n",
        "    momentum=0.9)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Define the dataset and data transform with flatten functions appended\n",
        "data_root = os.path.join(root_dir, 'data')\n",
        "train_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='train',\n",
        "    transform=data_transform)\n",
        "\n",
        "val_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='val',\n",
        "    transform=data_transform,\n",
        "    label_dict=train_dataset.label_dict)\n",
        "\n",
        "# Define the batch size and number of workers\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "\n",
        "# Define the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqtaJPLhD64R"
      },
      "source": [
        "I got an accuracy of 40.45% using my own implementation. How about you?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsGFydepSssy"
      },
      "source": [
        "Answing in 2-3 sentence. How does linear evaluation differ from finetuning, and when should we opt for it instead?\n",
        "\n",
        "Linear evalution consists of freezing the weights of the model aside from the last layer, which is trained only on the specific new training data. Fine-tuning does not involve freezing the weights. This can lead to more overfitting if the dataset is small or training time is limited. In this case, linear evaluation would make more sense, as it is less prone to this issue. We should opt for fine-tuning when the training data size is sufficiently large.\n",
        "\n",
        "[Your answer]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdSLohl5U_YE"
      },
      "source": [
        "### Finetune pretrained ResNet\n",
        "Fine-tuning a pre-trained ResNet model on a specific task, such as image classification or object detection, can help improve its performance on that task by adapting the model's learned representations to the particular dataset and task at hand. This is because the pre-trained ResNet has already learned useful feature representations on a large dataset, and fine-tuning allows it to further specialize those representations to the specific task.\n",
        "\n",
        "In this implementation, instead of freezing resnet features, we want to finetune it. Implement the \"finetune\" mode in Resnet class in model.py. The implementation is the same as the \"linear\" mode, except that we do not need to set requires_grad to False to freeze the features. Here, we are going to finetune the ResNet using different strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BUZNBNrWuJ5"
      },
      "source": [
        "#### Fully finetune with same learning rate\n",
        "End-to-end finetune with SGD optimizer. Setting lr to 0.01 and momentum to 0.9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9ioTgrPVmLMf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "bb00d6b1-6283-4503-b82e-0ed19ba4d70c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2:   0%|          | 0/1563 [00:02<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-56531d1bd9d1>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet_finetune1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-3cbf55430f53>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;31m# Backpropagate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;31m# Update the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "# Define the model, optimizer, and criterion (loss_fn)\n",
        "resnet_finetune1 = Resnet(mode='finetune',pretrained=True)\n",
        "\n",
        "optimizer = None\n",
        "optimizer = torch.optim.SGD(resnet_finetune1.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Define the dataset and data transform with flatten functions appended\n",
        "data_root = os.path.join(root_dir, 'data')\n",
        "train_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='train',\n",
        "    transform=data_transform)\n",
        "\n",
        "val_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='val',\n",
        "    transform=data_transform,\n",
        "    label_dict=train_dataset.label_dict)\n",
        "\n",
        "# Define the batch size and number of workers\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "\n",
        "# Define the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "train(resnet_finetune1, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZXvlFvmMkEW"
      },
      "source": [
        "I got an accuracy of 45.62% using my own implementation. How about you?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktHS7xCMWzEL"
      },
      "source": [
        "#### Fully finetune with different learning rates\n",
        "\n",
        "Lower layers in a network typically learn low-level features such as edges and textures that are useful across a wide range of tasks, and therefore may require smaller updates to prevent overfitting. Meanwhile, higher layers may learn more task-specific features that require larger updates to improve performance.\n",
        "\n",
        "Here, we finetune the pretrained ResNet with SGD optimizer with momentum = 0.9. We will need to assign different learning rate to different layers.\n",
        "\n",
        "\n",
        "*   Setting lr to 0.01 for the last fc layer.\n",
        "*   Setting lr to 0.001 for the rest layers.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Rid4RJYmnrG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f32993db-b404-4a12-a5f7-20e6b896163b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2: 100%|██████████| 1563/1563 [02:21<00:00, 11.07it/s, loss=2.42]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.8840, Accuracy = 0.4962\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2: 100%|██████████| 1563/1563 [02:19<00:00, 11.20it/s, loss=1.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 1.8990, Accuracy = 0.5016\n"
          ]
        }
      ],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "# Define the model, optimizer, and criterion (loss_fn)\n",
        "resnet_finetune2 = Resnet(mode='finetune',pretrained=True)\n",
        "\n",
        "optimizer = None\n",
        "# https://pytorch.org/docs/stable/generated/torch.optim.SGD.html\n",
        "optimizer = torch.optim.SGD([\n",
        "    {'params': [resnet_finetune2.get_parameter('resnet.fc.weight'), resnet_finetune2.get_parameter('resnet.fc.bias')], 'lr': 0.01},\n",
        "    {'params': [param for name, param in resnet_finetune2.named_parameters() if \"fc\" not in name], 'lr': 0.001}\n",
        "    ], momentum=0.9)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Define the dataset and data transform with flatten functions appended\n",
        "data_root = os.path.join(root_dir, 'data')\n",
        "train_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='train',\n",
        "    transform=data_transform)\n",
        "\n",
        "val_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='val',\n",
        "    transform=data_transform,\n",
        "    label_dict=train_dataset.label_dict)\n",
        "\n",
        "# Define the batch size and number of workers\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "\n",
        "# Define the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "train(resnet_finetune2, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh5yf5ZoZLUf"
      },
      "source": [
        "I got an accuracy of 47.23% using my own implementation. How about you?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwD8qM8OXNdC"
      },
      "source": [
        "#### Fintune with few forzen layers\n",
        "Freezing some layers during fine-tuning can help to prevent overfitting and speed up the training process. When we fine-tune a pre-trained neural network, we typically want to retain the learned feature representations in the lower layers of the network, which are often more general and transferable across different tasks. By freezing these lower layers, we prevent their weights from being updated during fine-tuning, which helps to ensure that the model retains its learned feature representations. This can be particularly important if we have a small amount of data available for the specific task we are fine-tuning for.\n",
        "\n",
        "In this step, you will define a new optimizer to forzen all the parameters in resnet.layer1 and resnet.layer2.\n",
        "\n",
        "Here, we finetune the pretrained ResNet with SGD optimizer with momentum = 0.9. We will need to assign different learning rate to different layers.\n",
        "\n",
        "\n",
        "*   Setting lr to 0.01 for the last fc layer.\n",
        "*   Setting lr to 0 for the resnet.layer1 and resnet.layer2 layers.\n",
        "*   Setting lr to 0.001 for the rest layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSSDYIAO7FuE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a83f9f7e-4bd0-46da-8dfd-15a246a65f20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Epoch 1/2:  96%|█████████▋| 1505/1563 [02:24<00:08,  7.12it/s, loss=2.57]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f97ecf4ae50>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
            "Exception ignored in:     self._shutdown_workers()<function _MultiProcessingDataLoaderIter.__del__ at 0x7f97ecf4ae50>\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
            "        self._shutdown_workers()\n",
            "if w.is_alive():  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
            "\n",
            "      File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
            "    if w.is_alive():assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
            "\n",
            "AssertionError    assert self._parent_pid == os.getpid(), 'can only test a child process': \n",
            "can only test a child processAssertionError\n",
            ": can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f97ecf4ae50>Exception ignored in: \n",
            "Traceback (most recent call last):\n",
            "<function _MultiProcessingDataLoaderIter.__del__ at 0x7f97ecf4ae50>  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
            "    \n",
            "self._shutdown_workers()Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
            "        self._shutdown_workers()if w.is_alive():\n",
            "\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
            "        if w.is_alive():assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
            "AssertionError    : assert self._parent_pid == os.getpid(), 'can only test a child process'can only test a child process\n",
            "\n",
            "AssertionError: can only test a child process\n",
            "Epoch 1/2: 100%|██████████| 1563/1563 [02:31<00:00, 10.34it/s, loss=2.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 2.5018, Accuracy = 0.4144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2:  93%|█████████▎| 1454/1563 [02:19<00:09, 10.95it/s, loss=2.16]Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f97ecf4ae50><function _MultiProcessingDataLoaderIter.__del__ at 0x7f97ecf4ae50>\n",
            "\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
            "Traceback (most recent call last):\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
            "        assert self._parent_pid == os.getpid(), 'can only test a child process'if w.is_alive():\n",
            "AssertionError\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process': \n",
            "AssertionErrorcan only test a child process\n",
            ": can only test a child process\n",
            "Exception ignored in: Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f97ecf4ae50>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
            "    <function _MultiProcessingDataLoaderIter.__del__ at 0x7f97ecf4ae50>self._shutdown_workers()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
            "  File \"/usr/lib/python3.8/multiprocessing/process.py\", line 160, in is_alive\n",
            "        assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionErrorAssertionError: can only test a child process\n",
            ": can only test a child process\n",
            "Epoch 2/2: 100%|██████████| 1563/1563 [02:29<00:00, 10.43it/s, loss=1.92]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 2.1421, Accuracy = 0.4727\n"
          ]
        }
      ],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "# Define the model, optimizer, and criterion (loss_fn)\n",
        "resnet_finetune3 = Resnet(mode='finetune',pretrained=True)\n",
        "\n",
        "\n",
        "layer1 = []\n",
        "layer2 = []\n",
        "\n",
        "for param_name in resnet_finetune3.named_parameters():\n",
        "  if \"resnet.layer1\" in param_name:\n",
        "    layer1.append(resnet_finetune3.get_parameter(param_name))\n",
        "  elif \"resnet.layer2\" in param_name:\n",
        "    layer2.append(resnet_finetune3.get_parameter(param_name))\n",
        "\n",
        "optimizer = torch.optim.SGD([\n",
        "    {'params': layer1, 'lr': 0.0},\n",
        "    {'params': layer2, 'lr': 0.0},\n",
        "    {'params': [resnet_finetune2.get_parameter('resnet.fc.weight'), resnet_finetune2.get_parameter('resnet.fc.bias')], 'lr': 0.01},\n",
        "    {'params': [param for name, param in resnet_finetune3.named_parameters() if name not in layer1 and name not in layer2 and \"fc\" not in name], 'lr': 0.001}\n",
        "    ], momentum=0.9)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Define the dataset and data transform with flatten functions appended\n",
        "data_root = os.path.join(root_dir, 'data')\n",
        "train_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='train',\n",
        "    transform=data_transform)\n",
        "\n",
        "val_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='val',\n",
        "    transform=data_transform,\n",
        "    label_dict=train_dataset.label_dict)\n",
        "\n",
        "# Define the batch size and number of workers\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "\n",
        "# Define the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "train(resnet_finetune3, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbYxF8Bd8uoe"
      },
      "source": [
        "#### Comparsion bewteen different method\n",
        "Q1: Which method perform better in method 1 and method 2. Give a explaination in two to three sentences.\n",
        "\n",
        "Method 2 performed better than Method 1. This means that in this case, it is very likely that method 1 was overfitting, and the learning rates for the lower weights should have been lowered.\n",
        "\n",
        "\n",
        "Q2: Which method perform better in method 2 and method 3. Method 2 performed better than Method 3. This likely means that there was no overfitting in Method 2, and the precautions taken in Method 3 to avoid overfitting instead caused underfitting.\n",
        "\n",
        "\n",
        "################# Your Answer #################################\n",
        "\n",
        "\n",
        "\n",
        "################# END of Your Answer ##########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty5OoHhESPrO"
      },
      "source": [
        "## Visulization\n",
        "The attention maps in Convolutional Neural Networks (CNNs) and Transformers are used to visualize the regions of an input image that the model is paying attention to when making predictions. However, the way these attention maps are computed and the information they convey are different."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohJx-EwATHEz"
      },
      "source": [
        "### CNN Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7wRpzJQHaAu"
      },
      "source": [
        "\n",
        "Now we will implement Prof. Zhou's famous paper: \"Learning Deep Features for Discriminative Localization\", in which he proposed class activation mapping (CAM). The main equation is:\n",
        "$$S_{c}=\\sum_{k} w_{k}^{c} \\sum_{x, y} f_{k}(x, y)=\\sum_{x, y} \\sum_{k} w_{k}^{c} f_{k}(x, y)$$\n",
        "where $f_{k}(x, y)$ represents the activation of unit k in the last convolutional layer, which is layer4 in resnet18.\n",
        "\n",
        "For more detailed implementation, please refer to the github repo: https://github.com/zhoubolei/CAM\n",
        "\n",
        "Please implement the CAM function in model.py. Specifically, given convolutional features, weights and a class index, CAM will output the reasons for classifying the image to the class, thus making CNN interpretable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "eE62mOwFEVhn"
      },
      "outputs": [],
      "source": [
        "def CAM(feature_conv, weight_softmax, class_idx):\n",
        "    \"\"\"\n",
        "    Implement CAM here\n",
        "    generate the class activation maps upsample to 256x256\n",
        "    refer to: https://github.com/zhoubolei/CAM\n",
        "    \"\"\"\n",
        "    output_cam = []\n",
        "    size_upsample = (256, 256)\n",
        "    batch_size, channels, height, width = feature_conv.shape\n",
        "    output_cam = []\n",
        "    for idx in class_idx:\n",
        "        cam = weight_softmax[idx].dot(feature_conv.reshape((channels, height*width)))\n",
        "        cam = cam.reshape(height, width)\n",
        "        cam = cam - np.min(cam)\n",
        "        cam_img = cam / np.max(cam)\n",
        "        cam_img = np.uint8(255 * cam_img)\n",
        "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
        "    return output_cam\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "FctjCANGDiw8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "1cf4a809-01a0-4af2-c463-9cb03e1f92cf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-35e9287f4c2a>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mseed_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresnet_finetune3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfinalconv_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'layer4'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'resnet_finetune3' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "seed_everything(0)\n",
        "resnet_finetune3.eval()\n",
        "\n",
        "finalconv_name = 'layer4'\n",
        "# hook the feature extractor\n",
        "features_blobs = []\n",
        "\n",
        "def hook_feature(module, input, output):\n",
        "    features_blobs.append(output.data.cpu().numpy())\n",
        "\n",
        "resnet_finetune3.resnet._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
        "\n",
        "params = list(resnet_finetune3.parameters())\n",
        "weight_softmax = np.squeeze(params[-2].data.cpu().numpy())\n",
        "\n",
        "classes = []\n",
        "for i in range(100):\n",
        "  classes.append(str(i))\n",
        "\n",
        "val_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='val',\n",
        "    transform=data_transform,\n",
        "    label_dict=train_dataset.label_dict)\n",
        "# val_dataset1 = val_dataset[:20]\n",
        "val_dataset1 = torch.utils.data.Subset(val_dataset, list(range(0,10000,1000)))\n",
        "print(len(val_dataset1))\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset1, batch_size=1, num_workers=num_workers, shuffle=True)\n",
        "\n",
        "figure = plt.figure(figsize=(24, 6))\n",
        "cols, rows = 10, 2\n",
        "counter = 0\n",
        "\n",
        "for i, data in enumerate(tqdm((val_loader))):\n",
        "    features_blobs = []\n",
        "    img, label = data\n",
        "    img = img.cuda()\n",
        "\n",
        "    logit = resnet_finetune3(img)\n",
        "\n",
        "    h_x = F.softmax(logit, dim=1).data.squeeze()\n",
        "    probs, idx = h_x.sort(0, True)\n",
        "    probs = probs.cpu().numpy()\n",
        "    idx = idx.cpu().numpy()\n",
        "\n",
        "    # generate class activation mapping for the top1 prediction\n",
        "    CAMs = CAM(features_blobs[0], weight_softmax, [idx[0]])\n",
        "\n",
        "    img = img.squeeze().permute(1,2,0)\n",
        "    height, width, _ = img.shape\n",
        "    img = img.cpu() * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])\n",
        "    img = img.cpu().numpy()\n",
        "    heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
        "    heatmap = heatmap/255\n",
        "    result = heatmap * 0.7 + img * 0.3\n",
        "\n",
        "    figure.add_subplot(rows, cols, i+1)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(classes[label])\n",
        "    plt.imshow(img, cmap=\"gray\")\n",
        "\n",
        "\n",
        "    figure.add_subplot(rows, cols, i+11)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(classes[idx[0]])\n",
        "\n",
        "    plt.imshow(result, cmap=\"gray\")\n",
        "    counter +=1\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "532GJGCHF1rI"
      },
      "source": [
        "## Steps to build a ViT from scratch\n",
        "Vision Transformer (ViT) is a state-of-the-art neural network architecture for image classification tasks. Unlike traditional convolutional neural networks (CNNs), which have been the standard in computer vision for many years, ViT relies on a self-attention mechanism to extract features from images. This approach has shown to achieve competitive results on various benchmark datasets, while also offering the flexibility to handle tasks that require attention over long-range dependencies in images. ViT has quickly gained popularity in the computer vision community, and has spurred further research into the use of self-attention mechanisms in other areas of deep learning.\n",
        "\n",
        "You will implement the ViT model on the Miniplaces dataset.\n",
        "\n",
        "To implement ViT model for image classification, you will need to follow these steps：\n",
        "1.  Extract feature vectors from the input images using a trainable linear projection layer, which converts the 2D image patches into 1D feature vectors.\n",
        "2. Positional encoding: Add a learnable positional encoding to each feature vector, which provides spatial information to the model.\n",
        "3. Transformer encoder: Stack multiple Transformer encoder layers to process the encoded features, which allows the model to learn both local and global interactions between the image patches.\n",
        "4. Classification head: Add a classification head on top of the final encoded feature vector, which maps the learned representations to the corresponding class labels.\n",
        "5. Training and evaluation: Train the ViT model using an appropriate optimization algorithm and loss function, and evaluate its performance on the validation and testing sets.\n",
        "\n",
        "If you are not familiair with ViT model, then you can read our textbook [Transformers for Vision](https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html#fig-vit), or review our [discussion slides](https://drive.google.com/file/d/1RKSnE9MOAGBu9T-_2TaBEm4ASF189Fms/view)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3WejhKIF1rI"
      },
      "source": [
        "### Tokenization:\n",
        "At this step, we need to divide each image into a set of non-overlapping patches, and treat each patch as a token. This is the key step that distinguishes ViT from other computer vision models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yktdHkZrKPVw"
      },
      "source": [
        "#### Tokenize_image Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDPametLF1rJ"
      },
      "outputs": [],
      "source": [
        "def tokenize_image(img, patch_size=16, stride=16):\n",
        "    \"\"\"\n",
        "    Tokenize an image into non-overlapping image patches.\n",
        "    Args:\n",
        "        img (torch.Tensor): The input image with shape (C, H, W).\n",
        "        patch_size (int): The size of each patch.\n",
        "        stride (int): The stride of the sliding window.\n",
        "    Returns:\n",
        "        patches (torch.Tensor): The tokenized patches with shape (N, patch_size*patch_size*C).\n",
        "    \"\"\"\n",
        "\n",
        "    C, H, W = img.shape\n",
        "    patches = []\n",
        "    # Hints: write two for loop to loop over this image\n",
        "    # Each patch is flattened into a 1-dimensional vector and stacked into a\n",
        "    # tensor with shape (N, C * patch_size(H) * patch_size(W)), where N is the number of patches.\n",
        "    # We only consider the case image size can be modulo by the patch_size\n",
        "    for h in range(0, H, stride):\n",
        "      for w in range(0, W, stride):\n",
        "          patch = img[:, h:h+patch_size, w:w+patch_size]\n",
        "          patch = patch.permute(1,2,0)\n",
        "          patch = patch.flatten()\n",
        "          patches.append(patch)\n",
        "    patches = torch.stack(patches, dim=0)\n",
        "\n",
        "    print(patches.shape)\n",
        "    return patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVfIbK5tF1rJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "970c1e8a-c4d1-4b1d-b117-8028ae939599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 192])\n",
            "torch.Size([4, 3072])\n",
            "Good! For patch_size: 32, the output match\n",
            "torch.Size([16, 768])\n",
            "Good! For patch_size: 16, the output match\n",
            "torch.Size([64, 192])\n",
            "Good! For patch_size: 8, the output match\n",
            "torch.Size([256, 48])\n",
            "Good! For patch_size: 4, the output match\n",
            "torch.Size([1024, 12])\n",
            "Good! For patch_size: 2, the output match\n"
          ]
        }
      ],
      "source": [
        "# test your implementation of tokenize_image\n",
        "random_img = torch.rand(3,64,64)\n",
        "patched_img = tokenize_image(random_img,8,8)\n",
        "\n",
        "for i in [32,16,8,4,2]:\n",
        "    out = tokenize_image(random_img,i,i)\n",
        "\n",
        "    fast_patch = Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = i, p2 = i)\n",
        "\n",
        "    answer = fast_patch(random_img.unsqueeze(0))\n",
        "    equal = torch.allclose(out,answer.squeeze(0))\n",
        "    #print('Difference: ', equal)\n",
        "    if equal:\n",
        "      print('Good! For patch_size: %d, the output match' %(i))\n",
        "    else:\n",
        "      print('Uh-oh! For patch_size: %d, the output are different' %(i))\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZo2ygYUKfWC"
      },
      "source": [
        "#### linear projection layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQcWs0XTIC7P"
      },
      "source": [
        "At this step, you will need to implement the linear projection linear project layer combined with tokenize operation.\n",
        "\n",
        "This layer is used to transfer a single image to the image embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZ9tqkaPK1g7"
      },
      "outputs": [],
      "source": [
        "class Tokenization_layer(nn.Module):\n",
        "  def __init__(self, dim, patch_dim,patch_height, patch_width):\n",
        "    super().__init__()\n",
        "    \"\"\"\n",
        "        Args:\n",
        "          dim (int): input and output dimension.\n",
        "          patch_dim(int): falttened vectot dimension for image patch\n",
        "          patch_height (int): height of one image patch\n",
        "          patch_weight (int): weight of one image patch\n",
        "\n",
        "        You can use Pytorch's built-in function and the above Rearrange method.\n",
        "        Input and output shapes of each layer:\n",
        "        1) Rerrange the image: (batch_size, channels, H,W) -> (batch_size,N,patch_dim)\n",
        "        2) Norm Layer1 (LayerNorm): (batch_size,N,patch_dim) -> (batch_size,N,patch_dim)\n",
        "        3) Linear Projection layer: (batch_size,N,patch_dim) -> (batch_size,N,dim)\n",
        "        4) Norm Layer2 (LayerNorm): (batch_size,N,dim)-> (batch_size,N,dim)\n",
        "    \"\"\"\n",
        "\n",
        "    self.to_patch = Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width)\n",
        "    self.norm1 = None\n",
        "    self.fc1 = None\n",
        "    self.norm2 = None\n",
        "\n",
        "    # Hints: You can use the Rearrange method above to achieve faster patch operation\n",
        "    # Append this layer to nn.Sequential\n",
        "    # Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width)\n",
        "    self.patch_dim = patch_dim\n",
        "    self.to_patch = nn.Sequential(\n",
        "        Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_height, p2=patch_width),\n",
        "    )\n",
        "    self.norm1 = nn.LayerNorm(patch_dim)\n",
        "    self.fc1 = nn.Linear(patch_dim, dim)\n",
        "    self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      x (torch.Tensor): input tensor in the shape of (batch_size,C,H,W)\n",
        "    Return:\n",
        "      out (torch.Tensor): output patch embedding tensor in the shape of (batch_size,N,dim)\n",
        "\n",
        "     The input tensor 'x' should pass through the following layers:\n",
        "     1) self.to_patch: Rerrange image\n",
        "     2) self.norm1: LayerNorm\n",
        "     3) self.fc1: Fully-Connected layer\n",
        "     4) self.norm2: LayerNorm\n",
        "\n",
        "    \"\"\"\n",
        "    out = None\n",
        "    x = self.to_patch(x)\n",
        "    x = self.norm1(x)\n",
        "    x = self.fc1(x)\n",
        "    x = self.norm2(x)\n",
        "\n",
        "    out  = x\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_root = os.path.join(root_dir, 'data')\n",
        "train_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='train',\n",
        "    transform=data_transform)\n",
        "\n",
        "matrix = torch.zeros(1000, 3, 128, 128)\n",
        "for i in range(1000):\n",
        "  matrix[i] = train_dataset[0][0]\n",
        "\n",
        "\n",
        "print(matrix.shape)\n",
        "# print((train_dataset[0][0]).shape)\n",
        "token = Tokenization_layer(192,192,8,8)\n",
        "token = token.forward(matrix)\n",
        "print(token.shape)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_Xmw8mDf_GD",
        "outputId": "aae88c08-394e-45a2-dbad-8b30ef36c42e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1000, 3, 128, 128])\n",
            "torch.Size([1000, 256, 192])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9b7ESVsF1rK"
      },
      "source": [
        "### Attention:\n",
        "You will need to follow the steps to implement multi-head attention in this question.\n",
        "1. **Obtain Q,K,V vectors**: To obtain the Q, K, and V vectors, the input vectors are processed through three distinct single linear layers. In our implementation, we use a single linear layer with 3xD output channels, and then we divide the output into three chunks. We consider the first chunk as the Q vectors, the second chunk as the K vectors, and the last chunk as the V vectors.\n",
        "\n",
        "2. **Calculate similarity**: Compute the similarity scores between query vectors and a set of key vectors using a dot product.\n",
        "\n",
        "3. **Apply softmax**: Apply a softmax function to normalize the similarity scores across the key vectors. This creates a probability distribution that represents the relative importance of each key vector with respect to the query vector.\n",
        "\n",
        "4. **Compute weighted sum**: Compute a weighted sum of the** value vectors**, where the weights are the probability distribution obtained in step 2. This produces a context vector that summarizes the most relevant information from the value vectors with respect to the query vector.\n",
        "\n",
        "5. **Concatenate output**: The outputs of each head are then concatenated and passed through another linear projection to produce the final output.\n",
        "\n",
        "For more details, you can read our [textbook](https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7er82fMhF1rK"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          dim (int): input and output dimension.\n",
        "          heads (int): number of attention heads.\n",
        "          dim_head (int): input dimension of each attention head.\n",
        "          dropout (float): dropout rate for attention and final_linear layer.\n",
        "\n",
        "        Initialize a attention block.\n",
        "        You can use Pytorch's built-in function.\n",
        "        Input and output shapes of each layer:\n",
        "        1) Define the inner dimension as number of heads* dimension of each head\n",
        "        2) to_qkv: (batch_size, dim) -> (batch_size,3*inner_dimension)\n",
        "        3) final_linear: (batch_size, inner_dim) -> (batch_size, dim)\n",
        "        \"\"\"\n",
        "\n",
        "        self.heads = heads\n",
        "        self.dim_head = None\n",
        "        self.to_qkv = None\n",
        "        self.dropout = None\n",
        "\n",
        "        self.inner_dim = dim_head *  heads\n",
        "        self.to_qkv = nn.Linear(dim, self.inner_dim * 3, bias = False)\n",
        "        self.dim_head = dim_head\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.final_linear = nn.Linear(self.inner_dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Forward pass of the attention block.\n",
        "        Args:\n",
        "            x (torch.Tensor): input tensor in the shape of (batch_size,N,dim).\n",
        "        Returns:\n",
        "            out (torch.Tensor): output tensor in the shape of (batch_size,N,dim).\n",
        "\n",
        "        The input tensor 'x' should pass through the following layers:\n",
        "        1) to_qkv: (batch_size,N,dim) -> (batch_size,N,3*inner_dimension)\n",
        "        2) Divide the ouput of to qkv to q,k,v and then divide them in to n heads\n",
        "            (batch_size,N,inner_dim) -> (batch_size,N,num_head,head_dim)\n",
        "        3) Use torch.matmul to get the product of q and k\n",
        "        4) Divide the above tensor by the squre root of head dimension\n",
        "        5) Apply softmax and then dropout on the above tensor\n",
        "        6) Mutiply the above tensor with v to get attention\n",
        "        7) Concatenate the attentions from multi-heads\n",
        "            (batch_size,N,num_head,head_dim) -> (batch_size,N,inner_dim)\n",
        "        8) Pass the output from last step to a fully connected layer\n",
        "        9) Apply dropout for the last step output\n",
        "        '''\n",
        "        out = None\n",
        "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
        "        scores = torch.matmul(q, k.transpose(-1, -2))\n",
        "        scores = scores / np.sqrt(self.dim_head)\n",
        "        scores = F.softmax(scores, dim=-1)\n",
        "        attention = torch.matmul(scores, v)\n",
        "        out = rearrange(attention, 'b h n d -> b n (h d)')\n",
        "        out = self.final_linear(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7yya6zjF1rL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f627915c-8cda-4246-9e18-bc61d41a7662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good! For input dim: 512, the output shape is correct\n",
            "Good! For input dim: 768, the output shape is correct\n",
            "Good! For input dim: 1096, the output shape is correct\n"
          ]
        }
      ],
      "source": [
        "# You can use this cell to check if the output shape of attention'\n",
        "for dim in [512,768,1096]:\n",
        "  test_tensor = torch.rand(2,196,dim)\n",
        "  att_layer = Attention(dim,8,64,0.4)\n",
        "  output_tensor = att_layer(test_tensor)\n",
        "  equal =  test_tensor.shape == output_tensor.shape\n",
        "  if equal:\n",
        "    print('Good! For input dim: %d, the output shape is correct' %(dim))\n",
        "  else:\n",
        "    print('Uh-oh! For input dim: %d, the output shape is wrong' %(dim))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRGAesmOfk0A"
      },
      "source": [
        "The norm layer in Vision Transformer (ViT) is a layer that performs layer normalization on the input. It is typically applied after the Multi-Head Attention (MHA) and the MLP layers in the ViT architecture. The norm layer is used to help the model learn better representations by ensuring that the activations are normalized and centered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFbmcWQCF1rL"
      },
      "outputs": [],
      "source": [
        "### PreNorm function\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        # keey the resiual connection here\n",
        "        return self.fn(self.norm(x), **kwargs)+x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l217s0ofF1rL"
      },
      "outputs": [],
      "source": [
        "#You can use\n",
        "a = PreNorm(768, Attention(768, heads = 8, dim_head = 64, dropout = 0.2))\n",
        "# to create a combination of layer norm and any other layer\n",
        "test_tensor = torch.rand(2,196,768)\n",
        "# you can use the following line to do the forward pass\n",
        "output_tensor = a(test_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De7Z8ywcF1rM"
      },
      "source": [
        "### PositionwiseFeedForward\n",
        "You will need to implement the posiotionwiseFeedForward layer in Vision Transformer.\n",
        "\n",
        "The FFN layer is called \"position-wise\" because it applies a separate feedforward network to each position in the sequence independently. It consists of two linear transformations with a non-linear activation function in between, typically GELU. The first linear transformation maps the input feature vector from its original dimension to a higher-dimensional space, and the second linear transformation maps it back to the original dimension. The output of the FFN layer is the element-wise sum of the input and the transformed feature vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zP9qXq3AF1rM"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, dim, mlp_dim, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        \"\"\"\n",
        "         Args:\n",
        "          dim (int): input and output dimension.\n",
        "          mlp_dim (int): the output dimension of the fist first layer.\n",
        "          dropout (float): dropout rate for both linear layers.\n",
        "\n",
        "        Initialize an MLP.\n",
        "        You can use Pytorch's built-in nn.Linear function.\n",
        "        Input and output sizes of each layer:\n",
        "          1) fc1: dim, mlp_dim\n",
        "          2) fc2: mlp_dim, dim\n",
        "        \"\"\"\n",
        "\n",
        "        self.fc1 = None\n",
        "        self.fc2 = None\n",
        "        self.dropout = None\n",
        "        self.activation = nn.GELU()\n",
        "        self.fc1 = nn.Linear(dim, mlp_dim)\n",
        "        self.fc2 = nn.Linear(mlp_dim, dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        Args:\n",
        "            x (torch.Tensor): input tensor in the shape of (batch_size,N,dim).\n",
        "        Returns:\n",
        "            out (torch.Tensor): output tensor in the shape of (batch_size,N,dim).\n",
        "\n",
        "        The input tensor 'x' should pass through the following layers:\n",
        "        1) fc1: (batch_size,N,dim) ->  (batch_size,N,mlp_dim)\n",
        "        2) Apply activation function\n",
        "        3) Apply dropout\n",
        "        3) fc2: (batch_size,N,mlp_dim) -> (batch_size,N,dim)\n",
        "        4) Apply dropout\n",
        "        '''\n",
        "\n",
        "        out = None\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        out = x\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o1D-3EsF1rM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd6c642f-73c9-43e2-e46c-eb681fc694df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good! For input dim: 512, the output shape is correct\n",
            "Good! For input dim: 768, the output shape is correct\n",
            "Good! For input dim: 1096, the output shape is correct\n"
          ]
        }
      ],
      "source": [
        "# You can use this cell to check if the output shape of PositionwiseFeedForward\n",
        "for dim in [512,768,1096]:\n",
        "  test_tensor = torch.rand(2,196,dim)\n",
        "  ffn = PositionwiseFeedForward(dim,dim*4,0.1)\n",
        "  output_tensor = ffn(test_tensor)\n",
        "  equal =  test_tensor.shape == output_tensor.shape\n",
        "  if equal:\n",
        "    print('Good! For input dim: %d, the output shape is correct' %(dim))\n",
        "  else:\n",
        "    print('Uh-oh! For input dim: %d, the output shape is wrong' %(dim))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoRkYfdoF1rM"
      },
      "source": [
        "### TransformerBlock\n",
        "Now you can follow the steps and use above class to implement the standard transformer block as demostrated in the following image.\n",
        "\n",
        " <img src=\"https://web.cs.ucla.edu/~smo3/cs188/assignment3/transformer_block.png\"  width=\"20%\" height=\"40%\">\n",
        "\n",
        "1. Apply Layer-norm to the input tensor\n",
        "2. Apply the Multi-Head Attention (MHA) layer to the output tensor from step1. The MHA layer takes in the input tensor, and returns the attention scores and the attention output tensor.\n",
        "3. Add the residual connection to the output of the MHA layer.\n",
        "4. Apply Layer-norm to output of last step\n",
        "5. Apply the Position-wise Feedforward Network (FFN) layer to the output of the previous step. The FFN layer takes in the output tensor, and returns the transformed output tensor.\n",
        "6. Add the residual connection to the output of the FFN layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMCLL9IJF1rN"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, heads, dim_head, mlp_dim, dropout = 0.):\n",
        "        \"Implements Transformer block.\"\n",
        "        super().__init__()\n",
        "        '''\n",
        "        Args:\n",
        "          dim (int): input and output dimension.\n",
        "          heads (int): number of attention heads.\n",
        "          dim_head (int): input dimension of each attention head.\n",
        "          mlp_dim (int):\n",
        "          dropout (float): dropout rate for attention and FFN layers.\n",
        "\n",
        "        '''\n",
        "        # Use the PreNorm,Attention and PositionwiseFeedForword class to build your\n",
        "        # Transformer block\n",
        "        self.attn = None\n",
        "        self.ff = None\n",
        "\n",
        "        self.attn = PreNorm(dim, Attention(dim, heads, dim_head, dropout))\n",
        "        self.ff = PreNorm(dim, PositionwiseFeedForward(dim, mlp_dim, dropout))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (torch.Tensor): input tensor in the shape of (batch_size,N,dim).\n",
        "        Returns:\n",
        "            out (torch.Tensor): output tensor in the shape of (batch_size,N,dim).\n",
        "        \"\"\"\n",
        "        x = self.attn(x)\n",
        "        x = self.ff(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLAFJkurF1rN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62dc3c9b-c276-43f1-be10-e43252338419"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good! For input dim: 512, the output shape is correct\n",
            "Good! For input dim: 768, the output shape is correct\n",
            "Good! For input dim: 1096, the output shape is correct\n"
          ]
        }
      ],
      "source": [
        "# You can use this cell to check if the output shape of Transformer\n",
        "for dim in [512,768,1096]:\n",
        "  test_tensor = torch.rand(2,196,dim)\n",
        "  transformer_block = Transformer(dim,8,64,dim*4,0.1)\n",
        "  output_tensor = transformer_block(test_tensor)\n",
        "  equal =  test_tensor.shape == output_tensor.shape\n",
        "  if equal:\n",
        "    print('Good! For input dim: %d, the output shape is correct' %(dim))\n",
        "  else:\n",
        "    print('Uh-oh! For input dim: %d, the output shape is wrong' %(dim))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYrPbuC3F1rN"
      },
      "source": [
        "### ViTModel\n",
        "Now you can use above classes to build your Vision Transfromer. Recall the ViT Architecture.\n",
        "\n",
        " <img src=\"https://web.cs.ucla.edu/~smo3/cs188/assignment3/vit.png\"  width=\"40%\" height=\"40%\">\n",
        "\n",
        " Recall the pipline for Vision Transformer model:\n",
        "\n",
        "1. Load the input images and preprocess them into a set of image patches. The patches should be non-overlapping and should cover the entire input image. Each patch should be flattened into a vector and projected into a lower-dimensional/equal-dimensional space using a linear layer.\n",
        "\n",
        "2. Add cls token and learnable positional embeddings to the projected patch vectors. The positional embedding should encode the spatial location of each patch in the input image.\n",
        "\n",
        "3. Stack several Transformer blocks to process the patch vectors. Each Transformer block should consist of a Multi-Head Attention (MHA) layer and a Position-wise Feedforward Network (FFN) layer, with residual connections and layer normalization applied after each layer.\n",
        "\n",
        "3. Apply a mean pooling operation over the output of the last Transformer block or take the output vector related to the cls token to obtain a fixed-size feature vector.\n",
        "\n",
        "5. Feed the feature vector into a fully-connected classification head to predict the class label of the input image.\n",
        "\n",
        "6. Train the model using a supervised learning objective, such as cross-entropy loss, and backpropagation to update the model weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h8oMh2vF1rN"
      },
      "outputs": [],
      "source": [
        "from operator import pos\n",
        "# helper method\n",
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    \"Implements Vision Transfromer\"\n",
        "    def __init__(self, *,\n",
        "                 image_size,\n",
        "                 patch_size,\n",
        "                 num_classes,\n",
        "                 dim,\n",
        "                 depth,\n",
        "                 heads,\n",
        "                 mlp_dim,\n",
        "                 pool = 'cls',\n",
        "                 channels = 3,\n",
        "                 dim_head = 64,\n",
        "                 dropout = 0.,\n",
        "                 emb_dropout = 0.,\n",
        "                ):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          image_size (int): the height/weight of the input image.\n",
        "          patch_size (int): image patch size. In the ViT paper, this value is 16.\n",
        "          num_classes (num_class): Number of image classes for MLP prediction head.\n",
        "          dim (int): patch and position embedding dimension.\n",
        "          depth (int): number of stacked transformer blocks.\n",
        "          heads (int): number of attention heads.\n",
        "          mlp_dim (int): inner dimension for MLP in transformer blocks.\n",
        "          pool (str): choice between \"cls\" and \"mean\".\n",
        "                      For cls, you will need to use the cls token for perdiction\n",
        "                      For mean, you will need to take the mean of last transformer output\n",
        "          channels (int): Input image channels. Set to 3 for RGB image.\n",
        "          dropout (float): dropout rate for transformer blocks.\n",
        "          emb_dropout (float): dropout rate for patch embedding.\n",
        "\n",
        "        \"\"\"\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = 0\n",
        "        patch_dim = 0\n",
        "\n",
        "        # TODO: Compute the num_patches and patch_dim\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        patch_dim = patch_height * patch_width * channels\n",
        "\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "        self.pool = pool\n",
        "        self.to_path_embedding = None\n",
        "\n",
        "        self.pos_embedding = None\n",
        "        self.cls_token = None\n",
        "        self.dropout = None\n",
        "        self.transformers = nn.ModuleList([])\n",
        "        self.mlp_head = None\n",
        "        # TODO:\n",
        "        # 1) Define self.to_path_embedding usinng the Tokenization_layer class\n",
        "        # 2) Define learnable 1-D pos_embedding using torch.randn, the number of\n",
        "        #    embedding should be num_patches+1\n",
        "        # 3) Define learnable 1-D cls_token with dimension = dim. You can use nn.Parameter\n",
        "        #    to define the learnable\n",
        "        # 4) Define dropout with emb_dropout\n",
        "        # 5) Define depth num of Transformers\n",
        "        # 6) Using nn.Sqeuential to create the MLP head including two layers:\n",
        "        #    The first layer in the MLP head is a LayerNorm layer.\n",
        "        #    The second layer in the MLP head is a linear layer change dimension to num_classes\n",
        "        # print(\"im over here\")\n",
        "        self.to_path_embedding = Tokenization_layer(dim,patch_dim,patch_height,patch_width)\n",
        "        self.pos_embedding = torch.randn(num_patches+1, dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "        for i in range(depth):\n",
        "          self.transformers.append(Transformer(dim, heads, dim_head, mlp_dim, dropout))\n",
        "        self.mlp_head = nn.Sequential(nn.LayerNorm(dim),nn.Linear(dim, num_classes))\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, img):\n",
        "        '''\n",
        "        Args:\n",
        "            x (torch.Tensor): input tensor in the shape of (batch_size,N,dim).\n",
        "        Returns:\n",
        "            out (torch.Tensor): output tensor in the shape of (batch_size,num_class).\n",
        "\n",
        "        The input tensor 'x' should pass through the following layers:\n",
        "        1) self.to_patch_embedding: (batch_size,C,H,W) -> (batch_size,N,dim)\n",
        "        2) Using torch.Tensor.repeat to repeat the cls alone batch dimension.\n",
        "           Then, concatenate with cls token (batch_size,N,dim) -> (batch_size,N+1,dim)\n",
        "        3) Take sum of patch embedding and position embedding, then apply dropout.\n",
        "        4) Passing through all the transformer blocks (batch_size,N+1,dim) -> (batch_size,N+1,dim)\n",
        "        5) Use cls token or use pool method to get latent code of batched images\n",
        "            (batch_size,N+1,dim) -> (batch_size,dim)\n",
        "        6) Apply layerNorm to the output of last step\n",
        "        7) Passing though the final mlp layers: (batch_size,dim) -> (batch_size,num_class)\n",
        "\n",
        "        '''\n",
        "        out = None\n",
        "        img = self.to_path_embedding.forward(img)\n",
        "        cls_token = self.cls_token.repeat(img.shape[0], 1, 1)\n",
        "        img = torch.cat((cls_token, img), dim=1)\n",
        "        img = img + self.pos_embedding.cuda()\n",
        "        img = self.dropout(img)\n",
        "\n",
        "        for transformer in self.transformers:\n",
        "          img = transformer(img)\n",
        "\n",
        "        if self.pool == 'cls':\n",
        "          out = img[:, 0]\n",
        "        elif self.pool == 'mean':\n",
        "          out = img.mean(dim=1)\n",
        "\n",
        "        out = self.mlp_head(out)\n",
        "        # out = None\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8oNroxoyG1r"
      },
      "source": [
        "Then let's train your ViT model with with cls token as pool policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tbp_mTZJF1rO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18cde75a-90c7-4ec7-ab40-f453cc679839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2: 100%|██████████| 1563/1563 [02:44<00:00,  9.48it/s, loss=3.79]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 3.8032, Accuracy = 0.1194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2: 100%|██████████| 1563/1563 [02:37<00:00,  9.90it/s, loss=3.23]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 3.6158, Accuracy = 0.1417\n"
          ]
        }
      ],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "#Define the model, optimizer, and criterion (loss_fn)\n",
        "model = ViT(image_size = 128,\n",
        "    patch_size = 16,\n",
        "    num_classes = 100,\n",
        "    dim = 192,\n",
        "    depth = 8,\n",
        "    heads = 4,\n",
        "    dim_head = 48,\n",
        "    mlp_dim = 768,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        "           )\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=1e-4,)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Define the dataset and data transform with flatten functions appended\n",
        "data_root = os.path.join(root_dir, 'data')\n",
        "train_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='train',\n",
        "    transform=data_transform)\n",
        "\n",
        "val_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='val',\n",
        "    transform=data_transform,\n",
        "    label_dict=train_dataset.label_dict)\n",
        "\n",
        "# Define the batch size and number of workers\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "\n",
        "# Define the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIF3rbTjxRqC"
      },
      "source": [
        "I got an accuracy of 14.43% using my own implementation. How about you?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGXNfFX9zEXb"
      },
      "source": [
        "Then let's train your ViT model with with average pooling as pool policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWLFLdbwy3ko",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4443a4b4-db63-4739-bfda-07eb67f1c8c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2: 100%|██████████| 1563/1563 [02:38<00:00,  9.86it/s, loss=3.74]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 3.8570, Accuracy = 0.1130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2: 100%|██████████| 1563/1563 [02:33<00:00, 10.18it/s, loss=3.36]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 3.6355, Accuracy = 0.1428\n"
          ]
        }
      ],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "#Define the model, optimizer, and criterion (loss_fn)\n",
        "model = ViT(image_size = 128,\n",
        "    patch_size = 16,\n",
        "    num_classes = 100,\n",
        "    dim = 192,\n",
        "    depth = 8,\n",
        "    heads = 4,\n",
        "    pool = 'mean',\n",
        "    dim_head = 48,\n",
        "    mlp_dim = 768,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        "           )\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=1e-4,)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Define the dataset and data transform with flatten functions appended\n",
        "data_root = os.path.join(root_dir, 'data')\n",
        "train_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='train',\n",
        "    transform=data_transform)\n",
        "\n",
        "val_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='val',\n",
        "    transform=data_transform,\n",
        "    label_dict=train_dataset.label_dict)\n",
        "\n",
        "# Define the batch size and number of workers\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "\n",
        "# Define the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIsqefHhZLUj"
      },
      "source": [
        "I got an accuracy of 14.11% using my own implementation. How about you?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRYdlXK51eE6"
      },
      "source": [
        "### Comparision bewteen ResNet and ViT\n",
        "Train your resnet18 model without pretrained weighted for 2 epoch, and comparing the accuracy with ViT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzBbgHQO2DF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22f4459a-14ca-4bf8-9fbc-bdd4ce912f1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "Epoch 1/2: 100%|██████████| 1563/1563 [02:28<00:00, 10.52it/s, loss=2.98]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 3.1021, Accuracy = 0.2313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2: 100%|██████████| 1563/1563 [02:33<00:00, 10.16it/s, loss=2.71]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set: Average loss = 2.8335, Accuracy = 0.2842\n"
          ]
        }
      ],
      "source": [
        "# Train your resnet18 model without pretrained weighted for 2 epoch, and comparing the accuracy with ViT model.\n",
        "seed_everything(0)\n",
        "\n",
        "# Define the model, optimizer, and criterion (loss_fn)\n",
        "resnet = Resnet(pretrained=False)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    resnet.parameters(),\n",
        "    lr=0.0001)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Define the dataset and data transform with flatten functions appended\n",
        "data_root = os.path.join(root_dir, 'data')\n",
        "train_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='train',\n",
        "    transform=data_transform)\n",
        "\n",
        "val_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='val',\n",
        "    transform=data_transform,\n",
        "    label_dict=train_dataset.label_dict)\n",
        "\n",
        "# Define the batch size and number of workers\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "\n",
        "# Define the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "train(resnet, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmYxmnjA2WCv"
      },
      "source": [
        "Use 2-3 sentence, explain the why there is a performance gap between ResNet and ViT when they are trained with a short time.\n",
        "\n",
        "When trained for a short time, ResNet's convolutional layers seem to do a better job of extracting meaningful patterns and pixel representations than ViT's attention mechanisms, which likely need a significant amount of training data and training time to establish proficient patterns within data. Attention needs to esablish patterns within global data, which is tough to do when the image is being segmented into patches. To reconcile, ViT needs a sufficient amount of data to extract value from attention."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "X42ej8ZhF1q4"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}